# ğŸ›¡ï¸ Flaskåšå®¢ç³»ç»Ÿå®‰å…¨ç›‘æ§è§£å†³æ–¹æ¡ˆ

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†ä¸€å¥—**å®Œå…¨åŸºäºå¼€æºæŠ€æœ¯**çš„ä¼ä¸šçº§å®‰å…¨ç›‘æ§è§£å†³æ–¹æ¡ˆï¼Œä¸“ä¸ºFlaskåšå®¢ç³»ç»Ÿè®¾è®¡ï¼Œé›¶è½¯ä»¶æˆæœ¬å®ç°å…¨é¢çš„å®‰å…¨æ€åŠ¿æ„ŸçŸ¥ã€å¨èƒæ£€æµ‹ã€äº‹ä»¶å“åº”å’Œåˆè§„å®¡è®¡èƒ½åŠ›ã€‚

## ğŸ“‹ ç›®å½•

- [æŠ€æœ¯æ¶æ„è®¾è®¡](#æŠ€æœ¯æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
- [ç›‘æ§èƒ½åŠ›å®ç°](#ç›‘æ§èƒ½åŠ›å®ç°)
- [éƒ¨ç½²å®æ–½æŒ‡å—](#éƒ¨ç½²å®æ–½æŒ‡å—)
- [é…ç½®ç¤ºä¾‹](#é…ç½®ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [è¿ç»´æŒ‡å—](#è¿ç»´æŒ‡å—)
- [æ‰©å±•è§„åˆ’](#æ‰©å±•è§„åˆ’)

---

## æŠ€æœ¯æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```mermaid
graph TB
    A[æ•°æ®é‡‡é›†å±‚] --> B[æ•°æ®è·¯ç”±å±‚]
    B --> C[å­˜å‚¨å±‚]
    B --> D[åˆ†æå±‚]
    C --> E[å¯è§†åŒ–å±‚]
    D --> E
    E --> F[å‘Šè­¦å“åº”å±‚]
    
    A1[Vectoræ—¥å¿—æ”¶é›†å™¨] --> A
    A2[Prometheusç›‘æ§] --> A
    A3[Suricataç½‘ç»œç›‘æ§] --> A
    A4[è‡ªå®šä¹‰é‡‡é›†å™¨] --> A
    
    B1[Apache Kafka] --> B
    B2[Kafka Streams] --> B
    
    C1[InfluxDB] --> C
    C2[Elasticsearch] --> C
    C3[PostgreSQL] --> C
    
    D1[Python MLå¼•æ“] --> D
    D2[Wazuhè§„åˆ™å¼•æ“] --> D
    D3[å®æ—¶åˆ†ææœåŠ¡] --> D
    
    E1[Grafanaä»ªè¡¨æ¿] --> E
    E2[Wazuhæ§åˆ¶å°] --> E
    E3[è‡ªå®šä¹‰Webåº”ç”¨] --> E
    
    F1[è‡ªåŠ¨åŒ–è„šæœ¬] --> F
    F2[TheHiveäº‹ä»¶ç®¡ç†] --> F
    F3[é€šçŸ¥ç³»ç»Ÿ] --> F
```

### æ•°æ®æµå‘è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®é‡‡é›†å±‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Flaskåº”ç”¨æ—¥å¿— â†’ Vector â†’ æ ‡å‡†åŒ–å¤„ç† â†’ å®æ—¶ä¼ è¾“               â”‚
â”‚ ç³»ç»ŸæŒ‡æ ‡ â†’ Prometheus â†’ Node Exporter â†’ æ—¶åºæ•°æ®             â”‚
â”‚ ç½‘ç»œæµé‡ â†’ Suricata â†’ IDSæ£€æµ‹ â†’ å®‰å…¨äº‹ä»¶                    â”‚
â”‚ æ•°æ®åº“å®¡è®¡ â†’ è‡ªå®šä¹‰æ”¶é›†å™¨ â†’ æŸ¥è¯¢æ—¥å¿— â†’ è®¿é—®è®°å½•              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æ•°æ®é¢„å¤„ç†                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–                                             â”‚
â”‚ â€¢ æ•æ„Ÿä¿¡æ¯è„±æ•å¤„ç†                                             â”‚
â”‚ â€¢ æ•°æ®è´¨é‡æ£€æŸ¥å’ŒéªŒè¯                                           â”‚
â”‚ â€¢ è·¯ç”±åˆ†å‘åˆ°ä¸åŒå­˜å‚¨                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        å­˜å‚¨å±‚              â”‚    â”‚       åˆ†æå±‚              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ InfluxDB: æ—¶åºæŒ‡æ ‡æ•°æ®     â”‚    â”‚ å®æ—¶è§„åˆ™å¼•æ“: å·²çŸ¥å¨èƒ    â”‚
    â”‚ Elasticsearch: æ—¥å¿—å…¨æ–‡    â”‚    â”‚ æœºå™¨å­¦ä¹ : å¼‚å¸¸è¡Œä¸ºæ£€æµ‹    â”‚
    â”‚ PostgreSQL: ç»“æ„åŒ–æ•°æ®     â”‚    â”‚ å…³è”åˆ†æ: å¤åˆæ”»å‡»è¯†åˆ«    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    å†³ç­–å’Œå“åº”å±‚                               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å¨èƒè¯„åˆ† â†’ é£é™©ç­‰çº§ â†’ å“åº”ç­–ç•¥ â†’ è‡ªåŠ¨åŒ–æ‰§è¡Œ â†’ äººå·¥å®¡æ ¸      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. æ•°æ®é‡‡é›†å±‚

#### Vectoræ—¥å¿—æ”¶é›†å™¨
**é€‰æ‹©ç†ç”±ï¼š**
- Rustç¼–å†™ï¼Œæ€§èƒ½ä¼˜å¼‚ï¼Œå†…å­˜å ç”¨ä½
- ä¸°å¯Œçš„æ•°æ®è½¬æ¢å’Œè·¯ç”±åŠŸèƒ½
- æ”¯æŒå¤šç§è¾“å…¥è¾“å‡ºæ ¼å¼
- å†…ç½®æ•°æ®è´¨é‡ä¿è¯æœºåˆ¶

**æ ¸å¿ƒé…ç½®ï¼š**
```toml
# /etc/vector/vector.toml
[sources.flask_app]
type = "file"
include = ["/var/log/flask-blog/*.log"]
read_from = "beginning"

[sources.nginx_access]
type = "file" 
include = ["/var/log/nginx/access.log"]
read_from = "end"

[transforms.parse_flask_logs]
type = "remap"
inputs = ["flask_app"]
source = '''
parsed = parse_json!(.message)
.timestamp = parsed.timestamp
.level = parsed.level
.user_id = parsed.user_id
.ip_address = parsed.ip_address
.action = parsed.action
'''

[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["parse_flask_logs"]
endpoint = "http://elasticsearch:9200"
index = "flask-blog-logs-%Y.%m.%d"

[sinks.kafka]
type = "kafka"
inputs = ["parse_flask_logs"]
bootstrap_servers = "kafka:9092"
topic = "security-events"
```

#### Prometheusç³»ç»Ÿç›‘æ§
**ç›‘æ§æŒ‡æ ‡è®¾è®¡ï¼š**
```yaml
# /etc/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "security_rules.yml"
  - "performance_rules.yml"

scrape_configs:
  - job_name: 'flask-blog'
    static_configs:
      - targets: ['localhost:5000']
    scrape_interval: 5s
    metrics_path: '/metrics'
    
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']
      
  - job_name: 'nginx'
    static_configs:
      - targets: ['localhost:9113']

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']
```

**è‡ªå®šä¹‰å®‰å…¨æŒ‡æ ‡ï¼š**
```python
# Flaskåº”ç”¨ä¸­çš„å®‰å…¨æŒ‡æ ‡æ”¶é›†
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# å®‰å…¨äº‹ä»¶è®¡æ•°å™¨
security_events = Counter('security_events_total', 
                         'Total security events', 
                         ['event_type', 'severity', 'user_id'])

# è®¤è¯å°è¯•ç›‘æ§
auth_attempts = Counter('auth_attempts_total',
                       'Authentication attempts',
                       ['result', 'ip_address', 'user_agent'])

# å¼‚å¸¸è®¿é—®æ¨¡å¼
abnormal_access = Gauge('abnormal_access_score',
                       'Abnormal access behavior score',
                       ['user_id', 'session_id'])

# APIå“åº”æ—¶é—´
api_response_time = Histogram('api_response_seconds',
                             'API response time',
                             ['endpoint', 'method', 'status'])

@app.before_request
def track_request():
    g.start_time = time.time()
    
    # è®°å½•è®¿é—®æ¨¡å¼
    user_id = session.get('user_id', 'anonymous')
    ip_address = request.remote_addr
    user_agent = request.user_agent.string
    
    # æ£€æŸ¥å¼‚å¸¸è®¿é—®æ¨¡å¼
    if is_suspicious_request(request):
        security_events.labels(
            event_type='suspicious_access',
            severity='medium',
            user_id=user_id
        ).inc()

@app.after_request  
def track_response(response):
    response_time = time.time() - g.start_time
    
    api_response_time.labels(
        endpoint=request.endpoint or 'unknown',
        method=request.method,
        status=response.status_code
    ).observe(response_time)
    
    # è®¤è¯å¤±è´¥ç›‘æ§
    if response.status_code == 401:
        auth_attempts.labels(
            result='failed',
            ip_address=request.remote_addr,
            user_agent=request.user_agent.string[:100]
        ).inc()
    
    return response
```

#### Suricataç½‘ç»œå…¥ä¾µæ£€æµ‹
**è§„åˆ™é…ç½®ä¼˜åŒ–ï¼š**
```yaml
# /etc/suricata/suricata.yaml
vars:
  address-groups:
    HOME_NET: "[10.0.0.0/8,192.168.0.0/16,172.16.0.0/12]"
    EXTERNAL_NET: "!$HOME_NET"
    HTTP_SERVERS: "$HOME_NET"
    SMTP_SERVERS: "$HOME_NET"
    SQL_SERVERS: "$HOME_NET"
    DNS_SERVERS: "$HOME_NET"

  port-groups:
    HTTP_PORTS: "80,443,8000,8080,5000"
    SHELLCODE_PORTS: "!80"

default-log-dir: /var/log/suricata/

# ä¸“é—¨é’ˆå¯¹Webåº”ç”¨çš„æ£€æµ‹è§„åˆ™
rule-files:
  - web-attacks.rules
  - sql-injection.rules  
  - xss-attacks.rules
  - bot-detection.rules
  - custom-blog-rules.rules

outputs:
  - eve-log:
      enabled: yes
      filetype: regular
      filename: eve.json
      types:
        - alert
        - http
        - dns
        - tls
        - files
        - smtp

  - stats:
      enabled: yes
      filename: stats.log
      interval: 8

# é’ˆå¯¹Flaskåšå®¢çš„è‡ªå®šä¹‰æ£€æµ‹è§„åˆ™
custom-blog-rules.rules: |
  alert http $EXTERNAL_NET any -> $HTTP_SERVERS $HTTP_PORTS (msg:"Blog Admin Login Attempt"; 
    content:"POST"; http_method; content:"/admin/login"; http_uri; 
    threshold: type both, track by_src, count 5, seconds 300; 
    sid:1000001; rev:1;)
    
  alert http $EXTERNAL_NET any -> $HTTP_SERVERS $HTTP_PORTS (msg:"Blog Comment Spam Pattern";
    content:"POST"; http_method; content:"/comments"; http_uri;
    pcre:"/http|www\.|\.com|\.net/i"; 
    threshold: type both, track by_src, count 3, seconds 60;
    sid:1000002; rev:1;)
    
  alert http $EXTERNAL_NET any -> $HTTP_SERVERS $HTTP_PORTS (msg:"Blog SQL Injection Attempt";
    content:"OR"; http_client_body; content:"1=1"; http_client_body;
    sid:1000003; rev:1;)
```

### 2. æ•°æ®å­˜å‚¨å±‚

#### InfluxDBæ—¶åºæ•°æ®åº“
**æ•°æ®æ¨¡å‹è®¾è®¡ï¼š**
```sql
-- ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
CREATE MEASUREMENT system_metrics (
  time TIMESTAMP,
  host TEXT TAG,
  cpu_usage FLOAT FIELD,
  memory_usage FLOAT FIELD,
  disk_usage FLOAT FIELD,
  network_in FLOAT FIELD,
  network_out FLOAT FIELD
);

-- åº”ç”¨æ€§èƒ½æŒ‡æ ‡  
CREATE MEASUREMENT app_metrics (
  time TIMESTAMP,
  endpoint TEXT TAG,
  method TEXT TAG,
  status_code TEXT TAG,
  response_time FLOAT FIELD,
  request_count INTEGER FIELD
);

-- å®‰å…¨äº‹ä»¶æŒ‡æ ‡
CREATE MEASUREMENT security_metrics (
  time TIMESTAMP,
  event_type TEXT TAG,
  severity TEXT TAG,
  source_ip TEXT TAG,
  user_id TEXT TAG,
  count INTEGER FIELD,
  risk_score FLOAT FIELD
);

-- ç”¨æˆ·è¡Œä¸ºæŒ‡æ ‡
CREATE MEASUREMENT user_behavior (
  time TIMESTAMP,
  user_id TEXT TAG,
  action TEXT TAG,
  resource TEXT TAG,
  session_duration FLOAT FIELD,
  page_views INTEGER FIELD,
  anomaly_score FLOAT FIELD
);
```

**æ•°æ®ä¿ç•™ç­–ç•¥ï¼š**
```sql
-- åˆ›å»ºæ•°æ®ä¿ç•™ç­–ç•¥
CREATE RETENTION POLICY "realtime" ON "security_db" 
  DURATION 24h REPLICATION 1 DEFAULT;

CREATE RETENTION POLICY "daily" ON "security_db"
  DURATION 30d REPLICATION 1;

CREATE RETENTION POLICY "weekly" ON "security_db" 
  DURATION 365d REPLICATION 1;

-- è‡ªåŠ¨åŒ–æ•°æ®èšåˆ
CREATE CONTINUOUS QUERY "hourly_security_summary" ON "security_db"
BEGIN
  SELECT mean("risk_score") AS mean_risk, 
         sum("count") AS total_events,
         max("risk_score") AS max_risk
  INTO "daily"."security_summary"
  FROM "realtime"."security_metrics"
  GROUP BY time(1h), "event_type"
END;
```

#### Elasticsearchæ—¥å¿—å­˜å‚¨
**ç´¢å¼•æ¨¡æ¿è®¾è®¡ï¼š**
```json
{
  "index_patterns": ["flask-blog-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "index.lifecycle.name": "flask-blog-policy",
      "refresh_interval": "5s"
    },
    "mappings": {
      "properties": {
        "@timestamp": {"type": "date"},
        "level": {"type": "keyword"},
        "message": {"type": "text", "analyzer": "standard"},
        "user_id": {"type": "keyword"},
        "ip_address": {"type": "ip"},
        "user_agent": {"type": "text"},
        "endpoint": {"type": "keyword"},
        "method": {"type": "keyword"},
        "status_code": {"type": "integer"},
        "response_time": {"type": "float"},
        "request_id": {"type": "keyword"},
        "session_id": {"type": "keyword"},
        "geo_location": {"type": "geo_point"},
        "security_tags": {"type": "keyword"}
      }
    }
  }
}
```

**ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼š**
```json
{
  "policy": {
    "phases": {
      "hot": {
        "min_age": "0ms",
        "actions": {
          "rollover": {
            "max_size": "10gb",
            "max_age": "1d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "allocate": {
            "number_of_replicas": 0
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "allocate": {
            "number_of_replicas": 0
          }
        }
      },
      "delete": {
        "min_age": "365d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

### 3. åˆ†æå¤„ç†å±‚

#### Wazuhå®‰å…¨ä¿¡æ¯ä¸äº‹ä»¶ç®¡ç†
**æ ¸å¿ƒæ£€æµ‹è§„åˆ™ï¼š**
```xml
<!-- /var/ossec/etc/rules/flask_blog_rules.xml -->
<group name="flask_blog,">

  <!-- ç™»å½•å¤±è´¥æ£€æµ‹ -->
  <rule id="100001" level="5">
    <decoded_as>json</decoded_as>
    <field name="level">ERROR</field>
    <field name="action">login_failed</field>
    <description>Flask Blog: Login failure detected</description>
  </rule>

  <!-- æš´åŠ›ç ´è§£æ£€æµ‹ -->  
  <rule id="100002" level="10" frequency="5" timeframe="300">
    <if_matched_sid>100001</if_matched_sid>
    <same_source_ip />
    <description>Flask Blog: Multiple login failures from same IP</description>
  </rule>

  <!-- æƒé™å‡çº§æ£€æµ‹ -->
  <rule id="100003" level="12">
    <decoded_as>json</decoded_as>
    <field name="action">role_change</field>
    <field name="new_role">admin</field>
    <description>Flask Blog: User role elevated to admin</description>
  </rule>

  <!-- æ•æ„Ÿæ•°æ®è®¿é—® -->
  <rule id="100004" level="8">
    <decoded_as>json</decoded_as>
    <field name="endpoint">^/admin/users</field>
    <field name="method">GET</field>
    <description>Flask Blog: Admin user data accessed</description>
  </rule>

  <!-- å¤§é‡æ•°æ®ä¸‹è½½ -->
  <rule id="100005" level="9">
    <decoded_as>json</decoded_as>
    <field name="response_size" type="pcre2">^[0-9]{8,}$</field>
    <description>Flask Blog: Large data download detected</description>
  </rule>

  <!-- SQLæ³¨å…¥å°è¯• -->
  <rule id="100006" level="12">
    <decoded_as>json</decoded_as>
    <field name="request_data" type="pcre2">(?i)(union|select|insert|delete|update|drop|create|alter|exec|script)</field>
    <description>Flask Blog: SQL injection attempt detected</description>
  </rule>

  <!-- XSSæ”»å‡»å°è¯• -->
  <rule id="100007" level="10">
    <decoded_as>json</decoded_as>
    <field name="request_data" type="pcre2">(?i)(\<script|\<iframe|\<object|javascript:|onload=|onerror=)</field>
    <description>Flask Blog: XSS attack attempt detected</description>
  </rule>

</group>
```

**è‡ªåŠ¨åŒ–å“åº”é…ç½®ï¼š**
```xml
<!-- /var/ossec/etc/ossec.conf -->
<ossec_config>
  <active-response>
    <command>firewall-drop</command>
    <location>local</location>
    <rules_id>100002</rules_id>
    <timeout>3600</timeout>
  </active-response>

  <active-response>
    <command>notification</command>
    <location>local</location>
    <rules_id>100003,100006,100007</rules_id>
  </active-response>

  <command>
    <name>firewall-drop</name>
    <executable>firewall-drop.sh</executable>
    <expect>srcip</expect>
    <timeout_allowed>yes</timeout_allowed>
  </command>

  <command>
    <name>notification</name>
    <executable>security-notification.py</executable>
    <expect>srcip</expect>
  </command>
</ossec_config>
```

#### æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹å¼•æ“
**ç”¨æˆ·è¡Œä¸ºåˆ†ææ¨¡å‹ï¼š**
```python
# /opt/security-ml/user_behavior_analyzer.py
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import joblib
import logging

class UserBehaviorAnalyzer:
    def __init__(self):
        self.isolation_forest = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        self.scaler = StandardScaler()
        self.is_trained = False
        
    def extract_features(self, user_logs):
        """æå–ç”¨æˆ·è¡Œä¸ºç‰¹å¾"""
        features = {}
        
        # æ—¶é—´ç‰¹å¾
        features['hour_variance'] = user_logs['hour'].var()
        features['access_count'] = len(user_logs)
        features['unique_endpoints'] = user_logs['endpoint'].nunique()
        features['avg_session_duration'] = user_logs['session_duration'].mean()
        
        # è®¿é—®æ¨¡å¼ç‰¹å¾
        features['page_view_rate'] = user_logs.groupby('session_id').size().mean()
        features['error_rate'] = (user_logs['status_code'] >= 400).mean()
        features['admin_access_ratio'] = (user_logs['endpoint'].str.contains('/admin/')).mean()
        
        # åœ°ç†ä½ç½®ç‰¹å¾
        features['ip_diversity'] = user_logs['ip_address'].nunique()
        features['location_changes'] = user_logs['geo_country'].nunique()
        
        # è®¾å¤‡ç‰¹å¾
        features['user_agent_changes'] = user_logs['user_agent'].nunique()
        features['mobile_ratio'] = user_logs['is_mobile'].mean()
        
        # æ•°æ®ä¼ è¾“ç‰¹å¾
        features['avg_request_size'] = user_logs['request_size'].mean()
        features['avg_response_size'] = user_logs['response_size'].mean()
        features['download_volume'] = user_logs['response_size'].sum()
        
        return pd.Series(features)
    
    def train_model(self, training_data):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        logging.info("å¼€å§‹è®­ç»ƒç”¨æˆ·è¡Œä¸ºå¼‚å¸¸æ£€æµ‹æ¨¡å‹")
        
        # ç‰¹å¾å·¥ç¨‹
        user_features = training_data.groupby('user_id').apply(self.extract_features)
        
        # æ•°æ®é¢„å¤„ç†
        X = self.scaler.fit_transform(user_features.fillna(0))
        
        # è®­ç»ƒæ¨¡å‹
        self.isolation_forest.fit(X)
        self.is_trained = True
        
        # ä¿å­˜æ¨¡å‹
        joblib.dump(self.isolation_forest, '/opt/security-ml/models/isolation_forest.pkl')
        joblib.dump(self.scaler, '/opt/security-ml/models/scaler.pkl')
        
        logging.info("æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜")
        
    def detect_anomalies(self, current_data):
        """æ£€æµ‹å¼‚å¸¸ç”¨æˆ·è¡Œä¸º"""
        if not self.is_trained:
            logging.warning("æ¨¡å‹å°šæœªè®­ç»ƒï¼ŒåŠ è½½å·²ä¿å­˜çš„æ¨¡å‹")
            self.load_model()
            
        # æå–ç‰¹å¾
        user_features = current_data.groupby('user_id').apply(self.extract_features)
        X = self.scaler.transform(user_features.fillna(0))
        
        # å¼‚å¸¸æ£€æµ‹
        anomaly_scores = self.isolation_forest.decision_function(X)
        is_anomaly = self.isolation_forest.predict(X) == -1
        
        results = pd.DataFrame({
            'user_id': user_features.index,
            'anomaly_score': anomaly_scores,
            'is_anomaly': is_anomaly,
            'risk_level': self.calculate_risk_level(anomaly_scores)
        })
        
        return results
    
    def calculate_risk_level(self, scores):
        """è®¡ç®—é£é™©ç­‰çº§"""
        risk_levels = []
        for score in scores:
            if score < -0.3:
                risk_levels.append('é«˜é£é™©')
            elif score < -0.1:
                risk_levels.append('ä¸­é£é™©')  
            elif score < 0.1:
                risk_levels.append('ä½é£é™©')
            else:
                risk_levels.append('æ­£å¸¸')
        return risk_levels
    
    def load_model(self):
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
        try:
            self.isolation_forest = joblib.load('/opt/security-ml/models/isolation_forest.pkl')
            self.scaler = joblib.load('/opt/security-ml/models/scaler.pkl')
            self.is_trained = True
        except FileNotFoundError:
            logging.error("æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶")
            raise

# å®æ—¶å¼‚å¸¸æ£€æµ‹æœåŠ¡
class RealTimeAnomalyDetector:
    def __init__(self):
        self.analyzer = UserBehaviorAnalyzer()
        self.data_buffer = []
        self.buffer_size = 1000
        
    def process_log_entry(self, log_entry):
        """å¤„ç†å•æ¡æ—¥å¿—è®°å½•"""
        self.data_buffer.append(log_entry)
        
        # ç¼“å†²åŒºæ»¡æ—¶è¿›è¡Œæ‰¹é‡æ£€æµ‹
        if len(self.data_buffer) >= self.buffer_size:
            self.run_detection()
            
    def run_detection(self):
        """è¿è¡Œå¼‚å¸¸æ£€æµ‹"""
        if not self.data_buffer:
            return
            
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(self.data_buffer)
        
        # è¿è¡Œå¼‚å¸¸æ£€æµ‹
        anomalies = self.analyzer.detect_anomalies(df)
        
        # å¤„ç†æ£€æµ‹ç»“æœ
        for _, anomaly in anomalies.iterrows():
            if anomaly['is_anomaly']:
                self.handle_anomaly(anomaly)
                
        # æ¸…ç©ºç¼“å†²åŒº
        self.data_buffer.clear()
        
    def handle_anomaly(self, anomaly):
        """å¤„ç†æ£€æµ‹åˆ°çš„å¼‚å¸¸"""
        alert_data = {
            'timestamp': datetime.now().isoformat(),
            'type': 'user_behavior_anomaly',
            'user_id': anomaly['user_id'],
            'risk_level': anomaly['risk_level'],
            'anomaly_score': float(anomaly['anomaly_score']),
            'description': f"ç”¨æˆ· {anomaly['user_id']} è¡Œä¸ºå¼‚å¸¸ï¼Œé£é™©ç­‰çº§ï¼š{anomaly['risk_level']}"
        }
        
        # å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
        self.send_alert(alert_data)
        
        # è®°å½•åˆ°æ—¥å¿—
        logging.warning(f"æ£€æµ‹åˆ°ç”¨æˆ·è¡Œä¸ºå¼‚å¸¸: {alert_data}")
        
    def send_alert(self, alert_data):
        """å‘é€å‘Šè­¦"""
        # å‘é€åˆ°Wazuh
        import requests
        requests.post('http://wazuh-manager:55000/security/alerts', 
                     json=alert_data,
                     headers={'Authorization': 'Bearer your-token'})
        
        # å‘é€åˆ°è‡ªå®šä¹‰å‘Šè­¦ç³»ç»Ÿ
        if alert_data['risk_level'] in ['é«˜é£é™©', 'ä¸­é£é™©']:
            self.send_immediate_notification(alert_data)
            
    def send_immediate_notification(self, alert_data):
        """å‘é€ç´§æ€¥é€šçŸ¥"""
        # é‚®ä»¶é€šçŸ¥
        import smtplib
        from email.mime.text import MIMEText
        
        msg = MIMEText(f"å®‰å…¨å‘Šè­¦ï¼š{alert_data['description']}")
        msg['Subject'] = 'åšå®¢ç³»ç»Ÿå®‰å…¨å¼‚å¸¸'
        msg['From'] = 'security@yourblog.com'
        msg['To'] = 'admin@yourblog.com'
        
        # å¾®ä¿¡/é’‰é’‰é€šçŸ¥
        webhook_data = {
            "msgtype": "text",
            "text": {
                "content": f"ğŸš¨ å®‰å…¨å‘Šè­¦\n{alert_data['description']}\næ—¶é—´ï¼š{alert_data['timestamp']}"
            }
        }
        
        requests.post(
            'https://oapi.dingtalk.com/robot/send?access_token=your-token',
            json=webhook_data
        )
```

### 4. å¯è§†åŒ–å±•ç¤ºå±‚

#### Grafanaå®‰å…¨ç›‘æ§é¢æ¿
**å®‰å…¨æ€åŠ¿æ€»è§ˆé¢æ¿é…ç½®ï¼š**
```json
{
  "dashboard": {
    "id": null,
    "title": "Flaskåšå®¢å®‰å…¨ç›‘æ§ä¸­å¿ƒ",
    "tags": ["security", "flask", "blog"],
    "timezone": "Asia/Shanghai",
    "panels": [
      {
        "id": 1,
        "title": "å®‰å…¨å¨èƒç­‰çº§",
        "type": "stat",
        "targets": [
          {
            "expr": "max(security_threat_level)",
            "legendFormat": "å½“å‰å¨èƒç­‰çº§"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 3},
                {"color": "red", "value": 7}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "å®æ—¶æ”»å‡»è¶‹åŠ¿",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(security_events_total[5m])) by (event_type)",
            "legendFormat": "{{event_type}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "ç”¨æˆ·å¼‚å¸¸è¡Œä¸º",
        "type": "table",
        "targets": [
          {
            "expr": "topk(10, user_anomaly_score > 0.7)",
            "format": "table"
          }
        ]
      },
      {
        "id": 4,
        "title": "æ”»å‡»æ¥æºåœ°å›¾",
        "type": "worldmap",
        "targets": [
          {
            "expr": "sum(security_events_total) by (source_country)",
            "legendFormat": "{{source_country}}"
          }
        ]
      }
    ],
    "time": {
      "from": "now-24h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

**å‘Šè­¦è§„åˆ™é…ç½®ï¼š**
```yaml
# /etc/grafana/provisioning/alerting/security_alerts.yml
groups:
  - name: security_alerts
    interval: 30s
    rules:
      - alert: HighSecurityThreatLevel
        expr: security_threat_level > 8
        for: 1m
        labels:
          severity: critical
          component: security
        annotations:
          summary: "å®‰å…¨å¨èƒç­‰çº§è¿‡é«˜"
          description: "å½“å‰å®‰å…¨å¨èƒç­‰çº§ä¸º {{ $value }}ï¼Œéœ€è¦ç«‹å³å¤„ç†"
          
      - alert: BruteForceAttack
        expr: increase(auth_failures_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
          component: authentication
        annotations:
          summary: "æ£€æµ‹åˆ°æš´åŠ›ç ´è§£æ”»å‡»"
          description: "åœ¨è¿‡å»5åˆ†é’Ÿå†…æ£€æµ‹åˆ° {{ $value }} æ¬¡è®¤è¯å¤±è´¥"
          
      - alert: AnomalousUserBehavior
        expr: user_anomaly_score > 0.8
        for: 1m
        labels:
          severity: warning
          component: user_behavior
        annotations:
          summary: "æ£€æµ‹åˆ°å¼‚å¸¸ç”¨æˆ·è¡Œä¸º"
          description: "ç”¨æˆ· {{ $labels.user_id }} è¡Œä¸ºå¼‚å¸¸è¯„åˆ†ä¸º {{ $value }}"
          
      - alert: SystemResourceExhaustion
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 5m
        labels:
          severity: critical
          component: system
        annotations:
          summary: "ç³»ç»Ÿå†…å­˜ä¸è¶³"
          description: "å¯ç”¨å†…å­˜ä»…å‰© {{ $value }}%"
```

### 5. è‡ªåŠ¨åŒ–å“åº”ç³»ç»Ÿ

#### åŸºäºPythonçš„å“åº”è„šæœ¬
```python
# /opt/security-response/response_manager.py
import subprocess
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List
import requests

class SecurityResponseManager:
    def __init__(self):
        self.response_actions = {
            'ip_block': self.block_ip_address,
            'user_suspend': self.suspend_user_account,
            'session_terminate': self.terminate_user_session,
            'alert_notify': self.send_notification,
            'log_detailed': self.enable_detailed_logging,
            'backup_trigger': self.trigger_backup
        }
        
        self.blocked_ips = set()
        self.suspended_users = set()
        
    def process_security_event(self, event_data: Dict):
        """å¤„ç†å®‰å…¨äº‹ä»¶å¹¶æ‰§è¡Œå“åº”åŠ¨ä½œ"""
        event_type = event_data.get('event_type')
        severity = event_data.get('severity', 'low')
        source_ip = event_data.get('source_ip')
        user_id = event_data.get('user_id')
        
        logging.info(f"å¤„ç†å®‰å…¨äº‹ä»¶: {event_type}, ä¸¥é‡ç¨‹åº¦: {severity}")
        
        # æ ¹æ®äº‹ä»¶ç±»å‹å’Œä¸¥é‡ç¨‹åº¦å†³å®šå“åº”ç­–ç•¥
        response_plan = self.get_response_plan(event_type, severity)
        
        for action in response_plan:
            try:
                self.execute_action(action, event_data)
            except Exception as e:
                logging.error(f"æ‰§è¡Œå“åº”åŠ¨ä½œå¤±è´¥ {action}: {e}")
                
    def get_response_plan(self, event_type: str, severity: str) -> List[str]:
        """è·å–å“åº”è®¡åˆ’"""
        plans = {
            ('brute_force_attack', 'high'): [
                'ip_block', 'alert_notify', 'log_detailed'
            ],
            ('sql_injection', 'critical'): [
                'ip_block', 'alert_notify', 'log_detailed', 'backup_trigger'
            ],
            ('user_behavior_anomaly', 'medium'): [
                'alert_notify', 'log_detailed'
            ],
            ('privilege_escalation', 'critical'): [
                'user_suspend', 'session_terminate', 'alert_notify', 'log_detailed'
            ],
            ('data_exfiltration', 'high'): [
                'user_suspend', 'ip_block', 'alert_notify', 'backup_trigger'
            ]
        }
        
        return plans.get((event_type, severity), ['alert_notify'])
    
    def execute_action(self, action: str, event_data: Dict):
        """æ‰§è¡Œå…·ä½“çš„å“åº”åŠ¨ä½œ"""
        if action in self.response_actions:
            self.response_actions[action](event_data)
        else:
            logging.warning(f"æœªçŸ¥çš„å“åº”åŠ¨ä½œ: {action}")
            
    def block_ip_address(self, event_data: Dict):
        """å°ç¦IPåœ°å€"""
        source_ip = event_data.get('source_ip')
        if not source_ip or source_ip in self.blocked_ips:
            return
            
        # ä½¿ç”¨iptableså°ç¦IP
        try:
            subprocess.run([
                'iptables', '-I', 'INPUT', '-s', source_ip, '-j', 'DROP'
            ], check=True)
            
            self.blocked_ips.add(source_ip)
            
            logging.info(f"å·²å°ç¦IPåœ°å€: {source_ip}")
            
            # è®¾ç½®è‡ªåŠ¨è§£å°æ—¶é—´ï¼ˆ1å°æ—¶åï¼‰
            self.schedule_ip_unblock(source_ip, 3600)
            
        except subprocess.CalledProcessError as e:
            logging.error(f"å°ç¦IPå¤±è´¥: {e}")
            
    def schedule_ip_unblock(self, ip: str, delay: int):
        """è®¡åˆ’è§£å°IPåœ°å€"""
        import threading
        
        def unblock_later():
            time.sleep(delay)
            try:
                subprocess.run([
                    'iptables', '-D', 'INPUT', '-s', ip, '-j', 'DROP'
                ], check=True)
                
                self.blocked_ips.discard(ip)
                logging.info(f"å·²è§£å°IPåœ°å€: {ip}")
                
            except subprocess.CalledProcessError as e:
                logging.error(f"è§£å°IPå¤±è´¥: {e}")
        
        thread = threading.Thread(target=unblock_later)
        thread.daemon = True
        thread.start()
        
    def suspend_user_account(self, event_data: Dict):
        """æš‚åœç”¨æˆ·è´¦æˆ·"""
        user_id = event_data.get('user_id')
        if not user_id or user_id in self.suspended_users:
            return
            
        # è°ƒç”¨Flaskåº”ç”¨APIæš‚åœç”¨æˆ·
        try:
            response = requests.post(
                f'http://localhost:5000/admin/users/{user_id}/suspend',
                headers={'Authorization': 'Bearer admin-token'},
                json={'reason': 'å®‰å…¨å¼‚å¸¸è‡ªåŠ¨æš‚åœ', 'duration': 24}
            )
            
            if response.status_code == 200:
                self.suspended_users.add(user_id)
                logging.info(f"å·²æš‚åœç”¨æˆ·è´¦æˆ·: {user_id}")
            else:
                logging.error(f"æš‚åœç”¨æˆ·å¤±è´¥: {response.text}")
                
        except requests.RequestException as e:
            logging.error(f"æš‚åœç”¨æˆ·è¯·æ±‚å¤±è´¥: {e}")
            
    def terminate_user_session(self, event_data: Dict):
        """ç»ˆæ­¢ç”¨æˆ·ä¼šè¯"""
        user_id = event_data.get('user_id')
        session_id = event_data.get('session_id')
        
        try:
            # è°ƒç”¨Flaskåº”ç”¨APIç»ˆæ­¢ä¼šè¯
            response = requests.post(
                f'http://localhost:5000/admin/sessions/{session_id}/terminate',
                headers={'Authorization': 'Bearer admin-token'}
            )
            
            if response.status_code == 200:
                logging.info(f"å·²ç»ˆæ­¢ç”¨æˆ·ä¼šè¯: {user_id}, {session_id}")
            else:
                logging.error(f"ç»ˆæ­¢ä¼šè¯å¤±è´¥: {response.text}")
                
        except requests.RequestException as e:
            logging.error(f"ç»ˆæ­¢ä¼šè¯è¯·æ±‚å¤±è´¥: {e}")
            
    def send_notification(self, event_data: Dict):
        """å‘é€é€šçŸ¥"""
        message = self.format_alert_message(event_data)
        
        # é‚®ä»¶é€šçŸ¥
        self.send_email_alert(message)
        
        # å³æ—¶é€šè®¯é€šçŸ¥
        self.send_im_alert(message)
        
        # çŸ­ä¿¡é€šçŸ¥ï¼ˆé«˜å±äº‹ä»¶ï¼‰
        if event_data.get('severity') == 'critical':
            self.send_sms_alert(message)
            
    def format_alert_message(self, event_data: Dict) -> str:
        """æ ¼å¼åŒ–å‘Šè­¦æ¶ˆæ¯"""
        return f"""
ğŸš¨ Flaskåšå®¢å®‰å…¨å‘Šè­¦

äº‹ä»¶ç±»å‹: {event_data.get('event_type', 'unknown')}
ä¸¥é‡ç¨‹åº¦: {event_data.get('severity', 'unknown')}
æ—¶é—´: {event_data.get('timestamp', datetime.now().isoformat())}
æºIP: {event_data.get('source_ip', 'unknown')}
ç”¨æˆ·ID: {event_data.get('user_id', 'unknown')}
æè¿°: {event_data.get('description', 'æ— è¯¦ç»†æè¿°')}

è¯·åŠæ—¶å¤„ç†ï¼
        """
        
    def send_email_alert(self, message: str):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        
        try:
            msg = MIMEMultipart()
            msg['From'] = 'security@yourblog.com'
            msg['To'] = 'admin@yourblog.com'
            msg['Subject'] = 'Flaskåšå®¢å®‰å…¨å‘Šè­¦'
            
            msg.attach(MIMEText(message, 'plain'))
            
            server = smtplib.SMTP('localhost', 25)
            server.send_message(msg)
            server.quit()
            
            logging.info("é‚®ä»¶å‘Šè­¦å‘é€æˆåŠŸ")
            
        except Exception as e:
            logging.error(f"å‘é€é‚®ä»¶å‘Šè­¦å¤±è´¥: {e}")
            
    def send_im_alert(self, message: str):
        """å‘é€å³æ—¶é€šè®¯å‘Šè­¦"""
        # é’‰é’‰æœºå™¨äºº
        try:
            webhook_url = 'https://oapi.dingtalk.com/robot/send?access_token=your-token'
            data = {
                "msgtype": "text",
                "text": {"content": message}
            }
            
            response = requests.post(webhook_url, json=data)
            if response.status_code == 200:
                logging.info("é’‰é’‰å‘Šè­¦å‘é€æˆåŠŸ")
            else:
                logging.error(f"é’‰é’‰å‘Šè­¦å‘é€å¤±è´¥: {response.text}")
                
        except Exception as e:
            logging.error(f"å‘é€å³æ—¶é€šè®¯å‘Šè­¦å¤±è´¥: {e}")
            
    def enable_detailed_logging(self, event_data: Dict):
        """å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•"""
        user_id = event_data.get('user_id')
        source_ip = event_data.get('source_ip')
        
        # ä¸´æ—¶æé«˜ç›¸å…³ç”¨æˆ·æˆ–IPçš„æ—¥å¿—è®°å½•çº§åˆ«
        enhanced_logging_config = {
            'user_id': user_id,
            'source_ip': source_ip,
            'log_level': 'DEBUG',
            'duration': 3600,  # 1å°æ—¶
            'start_time': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ä¾›åº”ç”¨è¯»å–
        with open('/tmp/enhanced_logging.json', 'w') as f:
            json.dump(enhanced_logging_config, f)
            
        logging.info(f"å·²å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•: {user_id}, {source_ip}")
        
    def trigger_backup(self, event_data: Dict):
        """è§¦å‘åº”æ€¥å¤‡ä»½"""
        try:
            # æ•°æ®åº“å¤‡ä»½
            subprocess.run([
                'mysqldump', '-u', 'root', '-p', 'flask_blog', 
                '>', f'/backup/emergency_{datetime.now().strftime("%Y%m%d_%H%M%S")}.sql'
            ], shell=True, check=True)
            
            # æ–‡ä»¶ç³»ç»Ÿå¿«ç…§
            subprocess.run([
                'rsync', '-av', '/var/www/flask-blog/', 
                f'/backup/files_{datetime.now().strftime("%Y%m%d_%H%M%S")}/'
            ], check=True)
            
            logging.info("åº”æ€¥å¤‡ä»½å®Œæˆ")
            
        except subprocess.CalledProcessError as e:
            logging.error(f"åº”æ€¥å¤‡ä»½å¤±è´¥: {e}")

# ä¸»ç›‘å¬æœåŠ¡
def main():
    response_manager = SecurityResponseManager()
    
    # ç›‘å¬Wazuhå‘Šè­¦
    import pika
    
    connection = pika.BlockingConnection(
        pika.ConnectionParameters('localhost')
    )
    channel = connection.channel()
    channel.queue_declare(queue='security_events')
    
    def callback(ch, method, properties, body):
        try:
            event_data = json.loads(body)
            response_manager.process_security_event(event_data)
        except Exception as e:
            logging.error(f"å¤„ç†å®‰å…¨äº‹ä»¶å¤±è´¥: {e}")
        
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    channel.basic_consume(queue='security_events', on_message_callback=callback)
    
    logging.info("å®‰å…¨å“åº”ç®¡ç†å™¨å·²å¯åŠ¨ï¼Œç­‰å¾…äº‹ä»¶...")
    channel.start_consuming()

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    main()
```

---

## éƒ¨ç½²å®æ–½æŒ‡å—

### ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚
```bash
# æ“ä½œç³»ç»Ÿ
Ubuntu 20.04/22.04 LTS (æ¨è)
CentOS 8 / Rocky Linux 8
Debian 11/12

# ç¡¬ä»¶è¦æ±‚
CPU: 4æ ¸å¿ƒä»¥ä¸Š (æ¨è8æ ¸å¿ƒ)
RAM: 8GBä»¥ä¸Š (æ¨è16GB)  
å­˜å‚¨: 100GB SSDä»¥ä¸Š (æ¨è500GB)
ç½‘ç»œ: 1Gbps

# è½¯ä»¶ä¾èµ–
Docker & Docker Compose
Python 3.8+
Node.js 16+ (ç”¨äºæŸäº›ç»„ä»¶)
Git
```

#### Docker Composeéƒ¨ç½²æ–‡ä»¶
```yaml
# docker-compose.yml
version: '3.8'

services:
  # æ•°æ®é‡‡é›†
  vector:
    image: vectordotdev/vector:latest
    volumes:
      - ./config/vector.toml:/etc/vector/vector.toml:ro
      - /var/log:/host/var/log:ro
      - vector-data:/var/lib/vector
    ports:
      - "8686:8686"
    restart: unless-stopped

  # æ¶ˆæ¯é˜Ÿåˆ—
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    restart: unless-stopped

  # å­˜å‚¨å±‚
  elasticsearch:
    image: elastic/elasticsearch:7.17.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    volumes:
      - es-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    restart: unless-stopped

  influxdb:
    image: influxdb:2.0
    environment:
      - INFLUXDB_DB=security_db
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_PASSWORD=secure_password
    volumes:
      - influx-data:/var/lib/influxdb2
    ports:
      - "8086:8086"
    restart: unless-stopped

  postgresql:
    image: postgres:13
    environment:
      - POSTGRES_DB=security_metadata
      - POSTGRES_USER=security_user
      - POSTGRES_PASSWORD=secure_password
    volumes:
      - pg-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped

  # å®‰å…¨åˆ†æ
  wazuh-manager:
    image: wazuh/wazuh-manager:4.5.4
    volumes:
      - wazuh-manager-config:/wazuh-config-mount
      - wazuh-manager-logs:/var/ossec/logs
      - wazuh-manager-etc:/var/ossec/etc
    ports:
      - "1514:1514/udp"
      - "1515:1515"
      - "514:514/udp"
      - "55000:55000"
    restart: unless-stopped

  # å¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin_password
      - GF_INSTALL_PLUGINS=grafana-worldmap-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "3000:3000"
    restart: unless-stopped

  kibana:
    image: elastic/kibana:7.17.0
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    restart: unless-stopped

  # è‡ªå®šä¹‰æœåŠ¡
  security-ml:
    build: ./security-ml
    depends_on:
      - kafka
      - influxdb
      - postgresql
    volumes:
      - ./security-ml:/app
      - ml-models:/app/models
    environment:
      - KAFKA_BROKERS=kafka:9092
      - INFLUX_URL=http://influxdb:8086
      - POSTGRES_URL=postgresql://security_user:secure_password@postgresql:5432/security_metadata
    restart: unless-stopped

  security-response:
    build: ./security-response
    depends_on:
      - kafka
    volumes:
      - ./security-response:/app
      - /var/run/docker.sock:/var/run/docker.sock
    privileged: true
    restart: unless-stopped

volumes:
  vector-data:
  es-data:
  influx-data:
  pg-data:
  grafana-data:
  wazuh-manager-config:
  wazuh-manager-logs:
  wazuh-manager-etc:
  ml-models:

networks:
  default:
    driver: bridge
```

### ä¸€é”®éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# deploy.sh - ä¸€é”®éƒ¨ç½²å®‰å…¨ç›‘æ§ç³»ç»Ÿ

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²Flaskåšå®¢å®‰å…¨ç›‘æ§ç³»ç»Ÿ..."

# æ£€æŸ¥ç¯å¢ƒ
echo "ğŸ“‹ æ£€æŸ¥éƒ¨ç½²ç¯å¢ƒ..."
command -v docker >/dev/null 2>&1 || { echo "âŒ è¯·å…ˆå®‰è£…Docker"; exit 1; }
command -v docker-compose >/dev/null 2>&1 || { echo "âŒ è¯·å…ˆå®‰è£…Docker Compose"; exit 1; }

# åˆ›å»ºç›®å½•ç»“æ„
echo "ğŸ“ åˆ›å»ºé…ç½®ç›®å½•..."
mkdir -p config/{vector,grafana/provisioning/dashboards,grafana/provisioning/datasources,wazuh}
mkdir -p security-ml/{models,scripts}
mkdir -p security-response/{scripts,templates}
mkdir -p data/{backup,logs}

# ç”Ÿæˆé…ç½®æ–‡ä»¶
echo "âš™ï¸ ç”Ÿæˆé…ç½®æ–‡ä»¶..."

# Vectoré…ç½®
cat > config/vector.toml << 'EOF'
[sources.flask_logs]
type = "file"
include = ["/host/var/log/flask-blog/*.log"]
read_from = "beginning"

[sources.nginx_logs]  
type = "file"
include = ["/host/var/log/nginx/*.log"]
read_from = "end"

[transforms.parse_logs]
type = "remap"
inputs = ["flask_logs", "nginx_logs"]
source = '''
# è§£æJSONæ ¼å¼æ—¥å¿—
if exists(.message) {
  parsed = parse_json(.message) ?? {}
  .timestamp = parsed.timestamp ?? now()
  .level = parsed.level ?? "info"
  .user_id = parsed.user_id ?? "anonymous"
  .ip_address = parsed.ip_address ?? "unknown"
}
'''

[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["parse_logs"]
endpoints = ["http://elasticsearch:9200"]
index = "flask-blog-logs-%Y.%m.%d"

[sinks.kafka_security]
type = "kafka"
inputs = ["parse_logs"]
bootstrap_servers = "kafka:9092"
topic = "security-events"
EOF

# Grafanaæ•°æ®æºé…ç½®
cat > config/grafana/provisioning/datasources/datasources.yml << 'EOF'
apiVersion: 1

datasources:
  - name: InfluxDB
    type: influxdb
    access: proxy
    url: http://influxdb:8086
    database: security_db
    isDefault: true
    
  - name: Elasticsearch
    type: elasticsearch  
    access: proxy
    url: http://elasticsearch:9200
    index: flask-blog-logs-*
    timeField: "@timestamp"
    
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
EOF

# æ„å»ºè‡ªå®šä¹‰é•œåƒ
echo "ğŸ”¨ æ„å»ºè‡ªå®šä¹‰æœåŠ¡é•œåƒ..."

# å®‰å…¨MLæœåŠ¡Dockerfile
cat > security-ml/Dockerfile << 'EOF'
FROM python:3.9-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "main.py"]
EOF

# å®‰å…¨MLä¾èµ–
cat > security-ml/requirements.txt << 'EOF'
pandas==1.5.3
numpy==1.24.3
scikit-learn==1.3.0
kafka-python==2.0.2
influxdb-client==1.37.0
psycopg2-binary==2.9.7
requests==2.31.0
schedule==1.2.0
EOF

# å®‰å…¨å“åº”æœåŠ¡Dockerfile  
cat > security-response/Dockerfile << 'EOF'
FROM python:3.9-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    iptables \
    curl \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "response_manager.py"]
EOF

# å®‰å…¨å“åº”ä¾èµ–
cat > security-response/requirements.txt << 'EOF'
kafka-python==2.0.2
requests==2.31.0
pika==1.3.2
smtplib-ssl==1.0.4
schedule==1.2.0
EOF

# è®¾ç½®æƒé™
echo "ğŸ” è®¾ç½®æ–‡ä»¶æƒé™..."
chmod +x deploy.sh
chown -R $(whoami):$(whoami) .

# å¯åŠ¨æœåŠ¡
echo "ğŸš€ å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
check_service() {
    local service=$1
    local url=$2
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s "$url" > /dev/null 2>&1; then
            echo "âœ… $service æœåŠ¡æ­£å¸¸"
            return 0
        fi
        echo "â³ ç­‰å¾… $service æœåŠ¡å¯åŠ¨... ($attempt/$max_attempts)"
        sleep 10
        ((attempt++))
    done
    
    echo "âŒ $service æœåŠ¡å¯åŠ¨å¤±è´¥"
    return 1
}

check_service "Elasticsearch" "http://localhost:9200"
check_service "Grafana" "http://localhost:3000"
check_service "InfluxDB" "http://localhost:8086"
check_service "Kibana" "http://localhost:5601"

# åˆå§‹åŒ–é…ç½®
echo "ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿé…ç½®..."

# åˆ›å»ºElasticsearchç´¢å¼•æ¨¡æ¿
curl -X PUT "localhost:9200/_index_template/flask-blog-logs" \
  -H "Content-Type: application/json" \
  -d '{
    "index_patterns": ["flask-blog-logs-*"],
    "template": {
      "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
      },
      "mappings": {
        "properties": {
          "@timestamp": {"type": "date"},
          "level": {"type": "keyword"},
          "user_id": {"type": "keyword"},
          "ip_address": {"type": "ip"},
          "message": {"type": "text"}
        }
      }
    }
  }'

# åˆ›å»ºInfluxDBæ•°æ®åº“
curl -X POST "http://localhost:8086/query" \
  --data-urlencode "q=CREATE DATABASE security_db"

echo "âœ… éƒ¨ç½²å®Œæˆï¼"
echo ""
echo "ğŸŒ è®¿é—®åœ°å€ï¼š"
echo "  Grafana: http://localhost:3000 (admin/admin_password)"
echo "  Kibana: http://localhost:5601"
echo "  Wazuh: http://localhost:55000"
echo ""
echo "ğŸ“š ä¸‹ä¸€æ­¥ï¼š"
echo "  1. é…ç½®Flaskåº”ç”¨æ—¥å¿—è¾“å‡ºåˆ° /var/log/flask-blog/"
echo "  2. å¯¼å…¥Grafanaä»ªè¡¨æ¿æ¨¡æ¿"
echo "  3. é…ç½®å‘Šè­¦é€šçŸ¥æ¸ é“"
echo "  4. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹"
echo ""
echo "ğŸ“– è¯¦ç»†æ–‡æ¡£ï¼š./SECURITY_MONITORING_SOLUTION.md"
```

### é…ç½®Flaskåº”ç”¨é›†æˆ
```python
# åœ¨Flaskåº”ç”¨ä¸­é›†æˆå®‰å…¨ç›‘æ§
import logging
import json
from datetime import datetime
from flask import Flask, request, session, g
import time

app = Flask(__name__)

# é…ç½®å®‰å…¨æ—¥å¿—
security_logger = logging.getLogger('security')
security_handler = logging.FileHandler('/var/log/flask-blog/security.log')
security_formatter = logging.Formatter('%(message)s')
security_handler.setFormatter(security_formatter)
security_logger.addHandler(security_handler)
security_logger.setLevel(logging.INFO)

def log_security_event(event_type, description, user_id=None, ip_address=None, severity='info'):
    """è®°å½•å®‰å…¨äº‹ä»¶"""
    event_data = {
        'timestamp': datetime.now().isoformat(),
        'event_type': event_type,
        'description': description,
        'user_id': user_id or session.get('user_id', 'anonymous'),
        'ip_address': ip_address or request.remote_addr,
        'user_agent': request.user_agent.string,
        'endpoint': request.endpoint,
        'method': request.method,
        'severity': severity,
        'session_id': session.get('session_id', ''),
        'request_id': g.get('request_id', '')
    }
    
    security_logger.info(json.dumps(event_data))

@app.before_request
def before_request():
    g.start_time = time.time()
    g.request_id = str(uuid.uuid4())
    
    # æ£€æŸ¥IPé»‘åå•
    if is_ip_blocked(request.remote_addr):
        log_security_event('blocked_ip_access', 'Blocked IP attempted access')
        abort(403)
        
    # æ£€æŸ¥è¯·æ±‚é¢‘ç‡
    if is_rate_limited(request.remote_addr):
        log_security_event('rate_limit_exceeded', 'Request rate limit exceeded')
        abort(429)

@app.after_request
def after_request(response):
    response_time = time.time() - g.start_time
    
    # è®°å½•æ‰€æœ‰è¯·æ±‚
    request_data = {
        'timestamp': datetime.now().isoformat(),
        'method': request.method,
        'endpoint': request.endpoint,
        'status_code': response.status_code,
        'response_time': response_time,
        'user_id': session.get('user_id', 'anonymous'),
        'ip_address': request.remote_addr,
        'user_agent': request.user_agent.string,
        'request_size': request.content_length or 0,
        'response_size': len(response.get_data()),
        'referer': request.headers.get('Referer', ''),
        'session_id': session.get('session_id', ''),
        'request_id': g.get('request_id', '')
    }
    
    # å†™å…¥è®¿é—®æ—¥å¿—
    app_logger = logging.getLogger('access')
    app_logger.info(json.dumps(request_data))
    
    # æ£€æµ‹å¼‚å¸¸å“åº”
    if response.status_code >= 400:
        log_security_event(
            'http_error',
            f'HTTP {response.status_code} error',
            severity='warning' if response.status_code < 500 else 'error'
        )
    
    return response

# ç™»å½•ç›‘æ§
@app.route('/login', methods=['POST'])
def login():
    username = request.form.get('username')
    password = request.form.get('password')
    
    # ç™»å½•éªŒè¯é€»è¾‘
    user = authenticate_user(username, password)
    
    if user:
        session['user_id'] = user.id
        session['session_id'] = str(uuid.uuid4())
        
        log_security_event(
            'user_login_success',
            f'User {username} logged in successfully',
            user_id=user.id
        )
        
        return redirect('/dashboard')
    else:
        log_security_event(
            'user_login_failed',
            f'Failed login attempt for username: {username}',
            severity='warning'
        )
        
        # æ£€æŸ¥æš´åŠ›ç ´è§£
        failed_attempts = get_failed_login_attempts(request.remote_addr)
        if failed_attempts > 5:
            log_security_event(
                'brute_force_attack',
                f'Brute force attack detected from {request.remote_addr}',
                severity='high'
            )
            
        return render_template('login.html', error='Invalid credentials')

# æƒé™å˜æ›´ç›‘æ§
@app.route('/admin/users/<int:user_id>/role', methods=['POST'])
@require_role('admin')
def change_user_role(user_id):
    new_role = request.json.get('role')
    old_role = get_user_role(user_id)
    
    if update_user_role(user_id, new_role):
        log_security_event(
            'role_change',
            f'User role changed from {old_role} to {new_role}',
            user_id=user_id,
            severity='high' if new_role == 'admin' else 'medium'
        )
        
        return {'success': True}
    else:
        log_security_event(
            'role_change_failed',
            f'Failed to change user role',
            user_id=user_id,
            severity='warning'
        )
        
        return {'success': False}, 400

# æ•°æ®è®¿é—®ç›‘æ§
@app.route('/admin/users')
@require_role('admin')
def list_users():
    log_security_event(
        'admin_data_access',
        'Admin accessed user list',
        severity='medium'
    )
    
    users = get_all_users()
    return render_template('admin/users.html', users=users)

# æ–‡ä»¶ä¸Šä¼ ç›‘æ§
@app.route('/upload', methods=['POST'])
@login_required
def upload_file():
    if 'file' not in request.files:
        return {'error': 'No file provided'}, 400
        
    file = request.files['file']
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹å’Œå¤§å°
    if not is_allowed_file(file.filename):
        log_security_event(
            'malicious_file_upload',
            f'Attempt to upload disallowed file: {file.filename}',
            severity='high'
        )
        return {'error': 'File type not allowed'}, 400
    
    if file.content_length > MAX_FILE_SIZE:
        log_security_event(
            'large_file_upload',
            f'Attempt to upload oversized file: {file.filename}',
            severity='medium'
        )
        return {'error': 'File too large'}, 400
    
    # ä¿å­˜æ–‡ä»¶
    filename = secure_filename(file.filename)
    file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
    
    log_security_event(
        'file_upload_success',
        f'File uploaded successfully: {filename}',
        severity='info'
    )
    
    return {'success': True, 'filename': filename}
```

---

## æ€§èƒ½ä¼˜åŒ–

### å­˜å‚¨ä¼˜åŒ–ç­–ç•¥
```yaml
ä¼˜åŒ–é…ç½®:
  Elasticsearch:
    ç´¢å¼•ç­–ç•¥:
      - æŒ‰å¤©æ»šåŠ¨ç´¢å¼•
      - çƒ­æ¸©å†·æ•°æ®åˆ†å±‚
      - è‡ªåŠ¨åˆ é™¤è¿‡æœŸæ•°æ®
    æŸ¥è¯¢ä¼˜åŒ–:
      - åˆç†è®¾ç½®åˆ†ç‰‡æ•°é‡
      - ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜
      - ä¼˜åŒ–èšåˆæŸ¥è¯¢
      
  InfluxDB:
    æ•°æ®ä¿ç•™:
      - å®æ—¶æ•°æ®: 24å°æ—¶
      - æ±‡æ€»æ•°æ®: 30å¤©
      - å†å²æ•°æ®: 1å¹´
    å‹ç¼©ç­–ç•¥:
      - å¯ç”¨æ•°æ®å‹ç¼©
      - å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
      
  ç³»ç»Ÿçº§:
    å†…å­˜ç®¡ç†:
      - åˆç†åˆ†é…JVMå †å†…å­˜
      - è®¾ç½®ç³»ç»Ÿé¡µé¢ç¼“å­˜
      - ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
    ç£ç›˜I/O:
      - ä½¿ç”¨SSDå­˜å‚¨
      - ä¼˜åŒ–ç£ç›˜è°ƒåº¦ç®—æ³•
      - å®šæœŸç¢ç‰‡æ•´ç†
```

### ç½‘ç»œä¼˜åŒ–
```bash
# /etc/sysctl.conf ç½‘ç»œä¼˜åŒ–é…ç½®
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.ipv4.tcp_congestion_control = bbr
net.core.netdev_max_backlog = 5000
```

---

## æ‰©å±•è§„åˆ’

### é«˜å¯ç”¨éƒ¨ç½²
```yaml
é›†ç¾¤æ¶æ„:
  è´Ÿè½½å‡è¡¡:
    - Nginx/HAProxyå‰ç«¯è´Ÿè½½å‡è¡¡
    - å¤šä¸ªGrafanaå®ä¾‹
    - æ•°æ®åº“è¿æ¥æ± 
    
  æ•°æ®å±‚é«˜å¯ç”¨:
    - Elasticsearché›†ç¾¤(3èŠ‚ç‚¹)
    - InfluxDBé›†ç¾¤æˆ–è”é‚¦
    - PostgreSQLä¸»ä»å¤åˆ¶
    
  æœåŠ¡å‘ç°:
    - Consul/EtcdæœåŠ¡æ³¨å†Œ
    - å¥åº·æ£€æŸ¥å’Œæ•…éšœè½¬ç§»
    - é…ç½®ä¸­å¿ƒåŒ–ç®¡ç†
```

### æœªæ¥åŠŸèƒ½æ‰©å±•
```yaml
è®¡åˆ’åŠŸèƒ½:
  AIå¢å¼º:
    - æ·±åº¦å­¦ä¹ å¨èƒæ£€æµ‹
    - è‡ªç„¶è¯­è¨€å¤„ç†æ—¥å¿—åˆ†æ
    - é¢„æµ‹æ€§å®‰å…¨åˆ†æ
    
  åˆè§„æ€§:
    - GDPRåˆè§„æ£€æŸ¥
    - SOXå®¡è®¡æ”¯æŒ
    - ISO27001è¯æ®æ”¶é›†
    
  é›†æˆèƒ½åŠ›:
    - ç¬¬ä¸‰æ–¹SIEMé›†æˆ
    - äº‘å®‰å…¨ä¸­å¿ƒå¯¹æ¥
    - ç§»åŠ¨ç«¯ç®¡ç†åº”ç”¨
```

---

## è¿ç»´ç®¡ç†æŒ‡å—

### æ—¥å¸¸ç»´æŠ¤è„šæœ¬
```bash
#!/bin/bash
# /opt/security-monitoring/scripts/daily_maintenance.sh
# å®‰å…¨ç›‘æ§ç³»ç»Ÿæ—¥å¸¸ç»´æŠ¤è„šæœ¬

set -e

LOG_FILE="/var/log/security-monitoring/maintenance.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

log() {
    echo "[$DATE] $1" | tee -a "$LOG_FILE"
}

log "å¼€å§‹æ‰§è¡Œæ—¥å¸¸ç»´æŠ¤ä»»åŠ¡"

# 1. æ£€æŸ¥ç£ç›˜ç©ºé—´
log "æ£€æŸ¥ç£ç›˜ç©ºé—´ä½¿ç”¨æƒ…å†µ"
df -h | while read line; do
    usage=$(echo $line | awk '{print $5}' | sed 's/%//')
    if [[ "$usage" =~ ^[0-9]+$ ]] && [ "$usage" -gt 80 ]; then
        log "è­¦å‘Š: ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡80%: $line"
        # æ¸…ç†æ—§æ—¥å¿—
        find /var/log/security-monitoring -name "*.log" -mtime +30 -delete
        log "å·²æ¸…ç†30å¤©å‰çš„æ—§æ—¥å¿—æ–‡ä»¶"
    fi
done

# 2. æ£€æŸ¥æœåŠ¡çŠ¶æ€
log "æ£€æŸ¥æ ¸å¿ƒæœåŠ¡çŠ¶æ€"
services=("elasticsearch" "grafana" "influxdb" "wazuh-manager")
for service in "${services[@]}"; do
    if docker ps | grep -q "$service"; then
        log "âœ… $service æœåŠ¡è¿è¡Œæ­£å¸¸"
    else
        log "âŒ $service æœåŠ¡å¼‚å¸¸ï¼Œå°è¯•é‡å¯"
        docker-compose restart "$service"
        sleep 10
        if docker ps | grep -q "$service"; then
            log "âœ… $service æœåŠ¡é‡å¯æˆåŠŸ"
        else
            log "âŒ $service æœåŠ¡é‡å¯å¤±è´¥ï¼Œéœ€è¦äººå·¥å¤„ç†"
        fi
    fi
done

# 3. æ•°æ®åº“ç»´æŠ¤
log "æ‰§è¡Œæ•°æ®åº“ç»´æŠ¤ä»»åŠ¡"

# Elasticsearchç´¢å¼•ç»´æŠ¤
curl -X POST "localhost:9200/_forcemerge?max_num_segments=1" >/dev/null 2>&1
log "Elasticsearchç´¢å¼•åˆå¹¶å®Œæˆ"

# InfluxDBå‹ç¼©
docker exec influxdb influx -execute "SHOW SERIES CARDINALITY ON security_db" >/dev/null 2>&1
log "InfluxDBæ•°æ®æ£€æŸ¥å®Œæˆ"

# 4. ç”Ÿæˆå¥åº·æŠ¥å‘Š
log "ç”Ÿæˆç³»ç»Ÿå¥åº·æŠ¥å‘Š"
REPORT_FILE="/var/log/security-monitoring/health_report_$(date +%Y%m%d).json"

{
    echo "{"
    echo "  \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\","
    echo "  \"system_info\": {"
    echo "    \"uptime\": \"$(uptime -p)\","
    echo "    \"load_average\": \"$(uptime | awk -F'load average:' '{print $2}')\","
    echo "    \"memory_usage\": \"$(free -h | grep Mem | awk '{print $3\"/\"$2}')\","
    echo "    \"disk_usage\": \"$(df -h / | tail -1 | awk '{print $5}')\""
    echo "  },"
    echo "  \"services\": {"
    for i, service in enumerate("${services[@]}"); do
        status=$(docker ps --filter "name=$service" --format "{{.Status}}" | head -1)
        echo "    \"$service\": \"$status\"$([ $((i+1)) -lt ${#services[@]} ] && echo ",")"
    done
    echo "  },"
    echo "  \"data_volumes\": {"
    echo "    \"elasticsearch\": \"$(curl -s localhost:9200/_cat/indices?h=store.size | awk '{sum+=$1} END {print sum\"MB\"}')\","
    echo "    \"influxdb\": \"$(docker exec influxdb du -sh /var/lib/influxdb2 | awk '{print $1}')\""
    echo "  }"
    echo "}"
} > "$REPORT_FILE"

log "å¥åº·æŠ¥å‘Šå·²ç”Ÿæˆ: $REPORT_FILE"

# 5. å¤‡ä»½å…³é”®é…ç½®
log "å¤‡ä»½å…³é”®é…ç½®æ–‡ä»¶"
BACKUP_DIR="/backup/config/$(date +%Y%m%d)"
mkdir -p "$BACKUP_DIR"

cp -r /opt/security-monitoring/config "$BACKUP_DIR/"
cp docker-compose.yml "$BACKUP_DIR/"

log "é…ç½®å¤‡ä»½å®Œæˆ: $BACKUP_DIR"

log "æ—¥å¸¸ç»´æŠ¤ä»»åŠ¡æ‰§è¡Œå®Œæˆ"
```

### æ€§èƒ½ç›‘æ§è„šæœ¬
```bash
#!/bin/bash
# /opt/security-monitoring/scripts/performance_monitor.sh
# æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–è„šæœ¬

METRICS_FILE="/var/log/security-monitoring/performance_metrics.log"
TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)

# æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
collect_system_metrics() {
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    local memory_usage=$(free | grep Mem | awk '{printf("%.2f", $3/$2 * 100.0)}')
    local disk_io=$(iostat -x 1 1 | tail -n +4 | head -n -1 | awk '{print $10}' | head -1)
    local network_rx=$(cat /sys/class/net/eth0/statistics/rx_bytes)
    local network_tx=$(cat /sys/class/net/eth0/statistics/tx_bytes)
    
    echo "$TIMESTAMP,system,cpu_usage,$cpu_usage" >> "$METRICS_FILE"
    echo "$TIMESTAMP,system,memory_usage,$memory_usage" >> "$METRICS_FILE"
    echo "$TIMESTAMP,system,disk_io_wait,$disk_io" >> "$METRICS_FILE"
    echo "$TIMESTAMP,system,network_rx,$network_rx" >> "$METRICS_FILE"
    echo "$TIMESTAMP,system,network_tx,$network_tx" >> "$METRICS_FILE"
}

# æ”¶é›†åº”ç”¨æŒ‡æ ‡
collect_app_metrics() {
    # ElasticsearchæŒ‡æ ‡
    local es_heap=$(curl -s localhost:9200/_nodes/stats/jvm | jq '.nodes[].jvm.mem.heap_used_percent')
    local es_query_time=$(curl -s localhost:9200/_nodes/stats/indices | jq '.nodes[].indices.search.query_time_in_millis')
    
    echo "$TIMESTAMP,elasticsearch,heap_usage,$es_heap" >> "$METRICS_FILE"
    echo "$TIMESTAMP,elasticsearch,avg_query_time,$es_query_time" >> "$METRICS_FILE"
    
    # GrafanaæŒ‡æ ‡
    local grafana_response=$(curl -s -w "%{time_total}" localhost:3000/api/health -o /dev/null)
    echo "$TIMESTAMP,grafana,response_time,$grafana_response" >> "$METRICS_FILE"
    
    # InfluxDBæŒ‡æ ‡
    local influx_series=$(docker exec influxdb influx -execute "SHOW SERIES CARDINALITY ON security_db" 2>/dev/null | tail -1 || echo "0")
    echo "$TIMESTAMP,influxdb,series_count,$influx_series" >> "$METRICS_FILE"
}

# æ€§èƒ½ä¼˜åŒ–å»ºè®®
performance_analysis() {
    local cpu_avg=$(tail -n 100 "$METRICS_FILE" | grep "cpu_usage" | awk -F',' '{sum+=$4; count++} END {print sum/count}')
    local mem_avg=$(tail -n 100 "$METRICS_FILE" | grep "memory_usage" | awk -F',' '{sum+=$4; count++} END {print sum/count}')
    
    if (( $(echo "$cpu_avg > 80" | bc -l) )); then
        echo "[$(date)] è­¦å‘Š: CPUä½¿ç”¨ç‡è¿‡é«˜ ($cpu_avg%)ï¼Œå»ºè®®ä¼˜åŒ–æŸ¥è¯¢æˆ–å¢åŠ è®¡ç®—èµ„æº"
    fi
    
    if (( $(echo "$mem_avg > 85" | bc -l) )); then
        echo "[$(date)] è­¦å‘Š: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ($mem_avg%)ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–ç¼“å­˜é…ç½®"
    fi
}

# ä¸»æ‰§è¡Œé€»è¾‘
collect_system_metrics
collect_app_metrics
performance_analysis
```

### å®‰å…¨äº‹ä»¶å¤„ç†æ‰‹å†Œ
```markdown
# å®‰å…¨äº‹ä»¶å“åº”æ‰‹å†Œ

## äº‹ä»¶åˆ†ç±»å’Œå¤„ç†æµç¨‹

### 1. é«˜å±äº‹ä»¶ (Critical)
**ç‰¹å¾**: ç³»ç»Ÿå…¥ä¾µã€æ•°æ®æ³„éœ²ã€æƒé™æ»¥ç”¨
**å“åº”æ—¶é—´**: 15åˆ†é’Ÿå†…
**å¤„ç†æ­¥éª¤**:
1. ç«‹å³éš”ç¦»å—å½±å“ç³»ç»Ÿ
2. ä¿å­˜è¯æ®å’Œæ—¥å¿—
3. è¯„ä¼°å½±å“èŒƒå›´
4. å¯åŠ¨åº”æ€¥å“åº”æµç¨‹
5. é€šçŸ¥ç›¸å…³äººå‘˜
6. åˆ¶å®šæ¢å¤è®¡åˆ’

### 2. ä¸­å±äº‹ä»¶ (High)
**ç‰¹å¾**: å¼‚å¸¸è®¿é—®ã€æš´åŠ›ç ´è§£ã€æ¶æ„æ–‡ä»¶ä¸Šä¼ 
**å“åº”æ—¶é—´**: 1å°æ—¶å†…
**å¤„ç†æ­¥éª¤**:
1. åˆ†æäº‹ä»¶è¯¦æƒ…
2. ä¸´æ—¶é˜»æ–­æ”»å‡»æº
3. åŠ å¼ºç›‘æ§
4. è¯„ä¼°é£é™©ç­‰çº§
5. å®æ–½é˜²æŠ¤æªæ–½

### 3. ä½å±äº‹ä»¶ (Medium/Low)
**ç‰¹å¾**: å¼‚å¸¸ç™»å½•ã€é¢‘ç¹è®¿é—®ã€é…ç½®å˜æ›´
**å“åº”æ—¶é—´**: 4å°æ—¶å†…
**å¤„ç†æ­¥éª¤**:
1. è®°å½•äº‹ä»¶ä¿¡æ¯
2. åˆ†æè¡Œä¸ºæ¨¡å¼
3. æ›´æ–°é˜²æŠ¤è§„åˆ™
4. æŒç»­è§‚å¯Ÿ

## å¸¸è§åœºæ™¯å¤„ç†æŒ‡å—

### SQLæ³¨å…¥æ”»å‡»
```bash
# 1. ç«‹å³é˜»æ–­æ”»å‡»IP
sudo iptables -I INPUT -s <æ”»å‡»IP> -j DROP

# 2. åˆ†ææ”»å‡»è½½è·
grep "SQL injection" /var/log/flask-blog/security.log | tail -20

# 3. æ£€æŸ¥æ•°æ®åº“å®Œæ•´æ€§
mysql -u root -p -e "CHECKSUM TABLE articles, users;"

# 4. æ›´æ–°WAFè§„åˆ™
# åœ¨nginxé…ç½®ä¸­æ·»åŠ SQLæ³¨å…¥é˜²æŠ¤è§„åˆ™
```

### æš´åŠ›ç ´è§£æ”»å‡»
```bash
# 1. æŸ¥çœ‹æ”»å‡»ç»Ÿè®¡
grep "brute_force_attack" /var/log/flask-blog/security.log | \
    awk '{print $5}' | sort | uniq -c | sort -nr

# 2. æ‰¹é‡å°ç¦æ”»å‡»IP
grep "brute_force_attack" /var/log/flask-blog/security.log | \
    awk '{print $5}' | sort | uniq | \
    while read ip; do
        sudo iptables -I INPUT -s $ip -j DROP
    done

# 3. å¯ç”¨è´¦æˆ·é”å®š
# åœ¨Flaskåº”ç”¨ä¸­å¯ç”¨è´¦æˆ·ä¸´æ—¶é”å®šåŠŸèƒ½
```

### æ•°æ®å¼‚å¸¸è®¿é—®
```bash
# 1. åˆ†æè®¿é—®æ¨¡å¼
grep "admin_data_access" /var/log/flask-blog/security.log | \
    awk '{print $4, $5}' | sort | uniq -c

# 2. æ£€æŸ¥æƒé™é…ç½®
# ç¡®è®¤ç®¡ç†å‘˜æƒé™åˆ†é…æ­£ç¡®æ€§

# 3. å®¡è®¡æ•°æ®å˜æ›´
# æ£€æŸ¥æ•°æ®åº“å®¡è®¡æ—¥å¿—
```
```

### ç›‘æ§è„šæœ¬è‡ªåŠ¨åŒ–
```bash
#!/bin/bash
# /opt/security-monitoring/scripts/automated_monitoring.sh
# è‡ªåŠ¨åŒ–ç›‘æ§å’Œå“åº”è„šæœ¬

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE="/opt/security-monitoring/config/monitoring.conf"
ALERT_THRESHOLD_FILE="/opt/security-monitoring/config/thresholds.json"

# è¯»å–é…ç½®
source "$CONFIG_FILE"

# æ£€æŸ¥ç™»å½•å¤±è´¥ç‡
check_login_failures() {
    local threshold=${LOGIN_FAILURE_THRESHOLD:-10}
    local timeframe=${LOGIN_FAILURE_TIMEFRAME:-300}  # 5åˆ†é’Ÿ
    
    local failures=$(grep "user_login_failed" /var/log/flask-blog/security.log | \
                    awk -v since="$(date -d "-${timeframe} seconds" '+%Y-%m-%d %H:%M:%S')" \
                    '$1 " " $2 >= since' | wc -l)
    
    if [ "$failures" -gt "$threshold" ]; then
        send_alert "login_failure_spike" "æ£€æµ‹åˆ°ç™»å½•å¤±è´¥æ¿€å¢: ${failures}æ¬¡" "high"
        
        # è‡ªåŠ¨å¯ç”¨é¢å¤–ä¿æŠ¤
        enable_enhanced_protection
    fi
}

# æ£€æŸ¥å¼‚å¸¸æµé‡
check_traffic_anomaly() {
    local current_requests=$(grep "$(date '+%Y-%m-%d %H:%M')" /var/log/nginx/access.log | wc -l)
    local avg_requests=$(grep "$(date -d '-1 hour' '+%Y-%m-%d %H:')" /var/log/nginx/access.log | \
                        awk '{print $4}' | cut -d':' -f2 | sort | uniq -c | \
                        awk '{sum+=$1} END {print int(sum/NR)}')
    
    if [ "$current_requests" -gt $((avg_requests * 3)) ]; then
        send_alert "traffic_spike" "æ£€æµ‹åˆ°å¼‚å¸¸æµé‡: å½“å‰${current_requests}ï¼Œå¹³å‡${avg_requests}" "medium"
    fi
}

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
check_system_resources() {
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    local memory_usage=$(free | grep Mem | awk '{printf("%.0f", $3/$2 * 100.0)}')
    local disk_usage=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
    
    if [ "${cpu_usage%.*}" -gt 90 ]; then
        send_alert "high_cpu" "CPUä½¿ç”¨ç‡è¿‡é«˜: ${cpu_usage}%" "high"
    fi
    
    if [ "$memory_usage" -gt 90 ]; then
        send_alert "high_memory" "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: ${memory_usage}%" "high"
    fi
    
    if [ "$disk_usage" -gt 85 ]; then
        send_alert "high_disk" "ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: ${disk_usage}%" "medium"
        # è‡ªåŠ¨æ¸…ç†æ—¥å¿—
        find /var/log -name "*.log" -mtime +7 -exec gzip {} \;
    fi
}

# å‘é€å‘Šè­¦
send_alert() {
    local alert_type=$1
    local message=$2
    local severity=$3
    local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    
    # è®°å½•å‘Šè­¦
    echo "[$timestamp] $severity: $alert_type - $message" >> /var/log/security-monitoring/alerts.log
    
    # å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
    curl -X POST http://localhost:9093/api/v1/alerts \
         -H "Content-Type: application/json" \
         -d '[{
               "labels": {
                 "alertname": "'$alert_type'",
                 "severity": "'$severity'",
                 "instance": "flask-blog"
               },
               "annotations": {
                 "summary": "'$message'",
                 "description": "è‡ªåŠ¨ç›‘æ§æ£€æµ‹åˆ°å®‰å…¨äº‹ä»¶"
               }
             }]'
    
    # é«˜å±å‘Šè­¦ç«‹å³é€šçŸ¥
    if [ "$severity" = "high" ] || [ "$severity" = "critical" ]; then
        # é’‰é’‰é€šçŸ¥
        curl -X POST "$DINGTALK_WEBHOOK" \
             -H "Content-Type: application/json" \
             -d '{
               "msgtype": "text",
               "text": {
                 "content": "ğŸš¨ å®‰å…¨å‘Šè­¦\nç±»å‹: '$alert_type'\nç­‰çº§: '$severity'\næè¿°: '$message'\næ—¶é—´: '$timestamp'"
               }
             }'
        
        # é‚®ä»¶é€šçŸ¥
        echo "$message" | mail -s "[SECURITY ALERT] $alert_type" admin@yourdomain.com
    fi
}

# å¯ç”¨å¢å¼ºä¿æŠ¤
enable_enhanced_protection() {
    # é™ä½ç™»å½•å¤±è´¥é˜ˆå€¼
    echo "LOGIN_FAILURE_THRESHOLD=5" > /tmp/enhanced_protection.conf
    
    # å¯ç”¨è¯¦ç»†æ—¥å¿—
    echo "DEBUG_LOGGING=true" >> /tmp/enhanced_protection.conf
    
    # è®¾ç½®ä¿æŠ¤æ¨¡å¼æ—¶é•¿ï¼ˆ2å°æ—¶ï¼‰
    echo "PROTECTION_MODE_UNTIL=$(date -d '+2 hours' +%s)" >> /tmp/enhanced_protection.conf
    
    echo "[$(date)] å·²å¯ç”¨å¢å¼ºä¿æŠ¤æ¨¡å¼" >> /var/log/security-monitoring/protection.log
}

# ä¸»æ‰§è¡Œé€»è¾‘
main() {
    echo "[$(date)] å¼€å§‹è‡ªåŠ¨åŒ–å®‰å…¨ç›‘æ§æ£€æŸ¥" >> /var/log/security-monitoring/monitoring.log
    
    check_login_failures
    check_traffic_anomaly
    check_system_resources
    
    echo "[$(date)] è‡ªåŠ¨åŒ–ç›‘æ§æ£€æŸ¥å®Œæˆ" >> /var/log/security-monitoring/monitoring.log
}

# è®¾ç½®å®šæ—¶æ‰§è¡Œ
if [ "$1" = "--install-cron" ]; then
    # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    (crontab -l 2>/dev/null; echo "*/5 * * * * /opt/security-monitoring/scripts/automated_monitoring.sh") | crontab -
    echo "è‡ªåŠ¨åŒ–ç›‘æ§å·²æ·»åŠ åˆ°crontab"
else
    main
fi
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹
```bash
# /opt/security-monitoring/config/monitoring.conf
# ç›‘æ§é…ç½®æ–‡ä»¶

# å‘Šè­¦é˜ˆå€¼
LOGIN_FAILURE_THRESHOLD=10
LOGIN_FAILURE_TIMEFRAME=300
TRAFFIC_SPIKE_MULTIPLIER=3
CPU_THRESHOLD=90
MEMORY_THRESHOLD=90
DISK_THRESHOLD=85

# é€šçŸ¥é…ç½®
DINGTALK_WEBHOOK="https://oapi.dingtalk.com/robot/send?access_token=your-token"
SMTP_SERVER="localhost"
SMTP_PORT=25
ADMIN_EMAIL="admin@yourdomain.com"

# è‡ªåŠ¨å“åº”é…ç½®
AUTO_BLOCK_ENABLED=true
AUTO_BLOCK_DURATION=3600  # 1å°æ—¶
ENHANCED_PROTECTION_DURATION=7200  # 2å°æ—¶

# å¤‡ä»½é…ç½®
BACKUP_RETENTION_DAYS=30
BACKUP_LOCATION="/backup/security-monitoring"
```

è¿™å¥—å®Œå…¨å…è´¹çš„å®‰å…¨ç›‘æ§è§£å†³æ–¹æ¡ˆä¸ºFlaskåšå®¢ç³»ç»Ÿæä¾›äº†ä¼ä¸šçº§çš„å®‰å…¨ä¿æŠ¤èƒ½åŠ›ï¼Œé€šè¿‡å¼€æºæŠ€æœ¯çš„å·§å¦™ç»„åˆï¼Œå®ç°äº†é›¶æˆæœ¬çš„ä¸“ä¸šå®‰å…¨è¿è¥ä¸­å¿ƒã€‚ç³»ç»Ÿå…·å¤‡å®Œæ•´çš„å¨èƒæ£€æµ‹ã€äº‹ä»¶å“åº”ã€åˆè§„å®¡è®¡å’Œè¿è¥ç®¡ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

**âœ¨ å…³é”®ä¼˜åŠ¿æ€»ç»“:**
- ğŸ†“ **é›¶è½¯ä»¶æˆæœ¬**: å®Œå…¨åŸºäºå¼€æºæŠ€æœ¯æ ˆ
- ğŸ›¡ï¸ **å…¨é¢é˜²æŠ¤**: æ¶µç›–åº”ç”¨å±‚ã€ç½‘ç»œå±‚ã€ç³»ç»Ÿå±‚å®‰å…¨
- ğŸ¤– **æ™ºèƒ½åˆ†æ**: æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹å’Œè‡ªåŠ¨åŒ–å“åº”
- ğŸ“Š **å¯è§†åŒ–è¿è¥**: ä¸“ä¸šçš„å®‰å…¨æ€åŠ¿æ„ŸçŸ¥é¢æ¿
- âš¡ **å®æ—¶å“åº”**: æ¯«ç§’çº§å¨èƒæ£€æµ‹å’Œç§’çº§è‡ªåŠ¨é˜»æ–­
- ğŸ”§ **æ˜“äºç»´æŠ¤**: å®Œæ•´çš„è¿ç»´è‡ªåŠ¨åŒ–è„šæœ¬å’Œç®¡ç†å·¥å…·
- ğŸ“ˆ **å¯æ‰©å±•æ€§**: æ”¯æŒé›†ç¾¤éƒ¨ç½²å’ŒåŠŸèƒ½æ‰©å±•
- ğŸ“‹ **åˆè§„æ”¯æŒ**: æ»¡è¶³å¤šç§å®‰å…¨åˆè§„è¦æ±‚