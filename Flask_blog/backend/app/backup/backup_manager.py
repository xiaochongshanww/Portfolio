"""
æ ¸å¿ƒå¤‡ä»½ç®¡ç†å™¨

æä¾›æ•°æ®åº“å¤‡ä»½ã€æ–‡ä»¶ç³»ç»Ÿå¿«ç…§ã€å¢é‡å¤‡ä»½ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import shutil
import sqlite3
import hashlib
import json
import tarfile
import gzip
import time
import threading
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any
from flask import current_app
from .. import db
from ..models import BackupRecord, BackupConfig, SHANGHAI_TZ
from .storage_manager import StorageManager


class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self):
        self.storage_manager = StorageManager()
        self.backup_base_dir = Path(current_app.config.get('BACKUP_BASE_DIR', 'backups'))
        self.backup_base_dir.mkdir(exist_ok=True)
        
        # çº¿ç¨‹å®‰å…¨çš„å–æ¶ˆæ ‡å¿—ç®¡ç†
        self._cancellation_flags = {}  # backup_id -> threading.Event
        self._flags_lock = threading.Lock()
        
        # æ´»è·ƒçš„å¤‡ä»½çº¿ç¨‹ç®¡ç†
        self._active_threads = {}  # backup_id -> threading.Thread
        
        # åˆ›å»ºå­ç›®å½•
        (self.backup_base_dir / 'database').mkdir(exist_ok=True)
        (self.backup_base_dir / 'files').mkdir(exist_ok=True)
        (self.backup_base_dir / 'snapshots').mkdir(exist_ok=True)
        (self.backup_base_dir / 'temp').mkdir(exist_ok=True)
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self._connection_pool_config = {
            'pool_size': 5,
            'max_overflow': 10,
            'pool_timeout': 30,
            'pool_recycle': 3600,
            'pool_pre_ping': True
        }
    
    def _create_cancellation_flag(self, backup_id: str) -> threading.Event:
        """åˆ›å»ºå–æ¶ˆæ ‡å¿—"""
        with self._flags_lock:
            cancel_event = threading.Event()
            self._cancellation_flags[backup_id] = cancel_event
            return cancel_event
    
    def _is_cancelled(self, backup_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ - å¢å¼ºç‰ˆï¼šæ”¯æŒæŒä¹…åŒ–å–æ¶ˆçŠ¶æ€"""
        # é¦–å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„æ ‡å¿—
        with self._flags_lock:
            cancel_event = self._cancellation_flags.get(backup_id)
            if cancel_event and cancel_event.is_set():
                return True
        
        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰æ ‡å¿—ï¼Œæ£€æŸ¥æ•°æ®åº“ä¸­çš„æŒä¹…åŒ–çŠ¶æ€
        try:
            backup_record = BackupRecord.query.filter_by(backup_id=backup_id).first()
            if backup_record and backup_record.status == 'cancelled':
                # æ•°æ®åº“ä¸­æ ‡è®°ä¸ºå·²å–æ¶ˆï¼ŒåŒæ—¶æ›´æ–°å†…å­˜æ ‡å¿—
                with self._flags_lock:
                    if backup_id not in self._cancellation_flags:
                        self._cancellation_flags[backup_id] = threading.Event()
                    self._cancellation_flags[backup_id].set()
                return True
        except Exception as e:
            current_app.logger.warning(f"æ£€æŸ¥æŒä¹…åŒ–å–æ¶ˆçŠ¶æ€å¤±è´¥ {backup_id}: {e}")
        
        return False
    
    def _set_cancelled(self, backup_id: str):
        """è®¾ç½®å–æ¶ˆæ ‡å¿— - å¢å¼ºç‰ˆï¼šæŒä¹…åŒ–åˆ°æ•°æ®åº“"""
        # è®¾ç½®å†…å­˜æ ‡å¿—
        with self._flags_lock:
            cancel_event = self._cancellation_flags.get(backup_id)
            if cancel_event:
                cancel_event.set()
            current_app.logger.info(f"Backup {backup_id} cancellation flag set")
        
        # æŒä¹…åŒ–å–æ¶ˆçŠ¶æ€åˆ°æ•°æ®åº“
        try:
            backup_record = BackupRecord.query.filter_by(backup_id=backup_id).first()
            if backup_record and backup_record.status in ['pending', 'running']:
                backup_record.status = 'cancelled'
                backup_record.error_message = 'ç”¨æˆ·å–æ¶ˆ'
                # ä¸è®¾ç½®completed_atï¼Œå› ä¸ºä»»åŠ¡è¢«å–æ¶ˆè€Œéå®Œæˆ
                db.session.commit()
                current_app.logger.info(f"Backup {backup_id} æŒä¹…åŒ–å–æ¶ˆçŠ¶æ€å·²ä¿å­˜åˆ°æ•°æ®åº“")
        except Exception as e:
            current_app.logger.error(f"æŒä¹…åŒ–å–æ¶ˆçŠ¶æ€å¤±è´¥ {backup_id}: {e}")
            db.session.rollback()
    
    def _cleanup_cancellation_flag(self, backup_id: str):
        """æ¸…ç†å–æ¶ˆæ ‡å¿—"""
        with self._flags_lock:
            self._cancellation_flags.pop(backup_id, None)
            self._active_threads.pop(backup_id, None)
    
    def cancel_backup(self, backup_id: str) -> bool:
        """å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„å¤‡ä»½ä»»åŠ¡ - å¢å¼ºç‰ˆï¼šæ”¯æŒæŒä¹…åŒ–å–æ¶ˆå’Œå¼ºåˆ¶ç»ˆæ­¢"""
        try:
            # æŸ¥æ‰¾å¤‡ä»½è®°å½•
            backup_record = BackupRecord.query.filter_by(backup_id=backup_id).first()
            if not backup_record:
                current_app.logger.warning(f"Backup record {backup_id} not found")
                return False
            
            # æ£€æŸ¥å¤‡ä»½çŠ¶æ€
            if backup_record.status not in ['pending', 'running']:
                current_app.logger.info(f"Backup {backup_id} cannot be cancelled, status: {backup_record.status}")
                return False
            
            # è®¾ç½®å–æ¶ˆæ ‡å¿—ï¼ˆåŒ…å«æŒä¹…åŒ–ï¼‰
            self._set_cancelled(backup_id)
            
            # å°è¯•å¼ºåˆ¶ç»ˆæ­¢æ´»è·ƒçš„çº¿ç¨‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            with self._flags_lock:
                active_thread = self._active_threads.get(backup_id)
                if active_thread and active_thread.is_alive():
                    current_app.logger.info(f"å‘ç°æ´»è·ƒçº¿ç¨‹ {backup_id}ï¼Œå·²è®¾ç½®å–æ¶ˆæ ‡å¿—ï¼Œçº¿ç¨‹å°†åœ¨ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹åœæ­¢")
                else:
                    current_app.logger.info(f"æœªå‘ç°æ´»è·ƒçº¿ç¨‹ {backup_id}ï¼Œå¯èƒ½ä»»åŠ¡å·²å®Œæˆæˆ–çº¿ç¨‹å·²ç»ˆæ­¢")
            
            # è®°å½•å–æ¶ˆæ“ä½œåˆ°æ—¥å¿—
            current_app.logger.info(f"Backup {backup_id} cancellation initiated - persistent flags set")
            
            return True
            
        except Exception as e:
            current_app.logger.error(f"Failed to cancel backup {backup_id}: {e}")
            return False
    
    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ä»¥é‡Šæ”¾ç£ç›˜ç©ºé—´"""
        try:
            current_app.logger.info("å¼€å§‹æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
            temp_dir = self.backup_base_dir / 'temp'
            if temp_dir.exists():
                for temp_file in temp_dir.iterdir():
                    try:
                        if temp_file.is_file():
                            # åˆ é™¤è¶…è¿‡1å°æ—¶çš„ä¸´æ—¶æ–‡ä»¶
                            if time.time() - temp_file.stat().st_mtime > 3600:
                                temp_file.unlink()
                                current_app.logger.info(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                    except Exception as e:
                        current_app.logger.warning(f"æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_file}: {e}")
        except Exception as e:
            current_app.logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_engine_with_pool(self, db_uri: str):
        """è·å–å¸¦è¿æ¥æ± çš„æ•°æ®åº“å¼•æ“ä»¥æé«˜æ€§èƒ½"""
        try:
            from sqlalchemy import create_engine
            return create_engine(
                db_uri, 
                **self._connection_pool_config,
                echo=False  # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—ä»¥æé«˜æ€§èƒ½
            )
        except Exception as e:
            current_app.logger.error(f"åˆ›å»ºæ•°æ®åº“è¿æ¥æ± å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºæœ¬è¿æ¥
            from sqlalchemy import create_engine
            return create_engine(db_uri)
    
    def monitor_backup_performance(self, start_time: float, backup_id: str, backup_type: str):
        """ç›‘æ§å¤‡ä»½æ€§èƒ½å¹¶è®°å½•æŒ‡æ ‡"""
        try:
            import time
            duration = time.time() - start_time
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            current_app.logger.info(
                f"å¤‡ä»½æ€§èƒ½ - ID: {backup_id}, ç±»å‹: {backup_type}, "
                f"è€—æ—¶: {duration:.2f}ç§’"
            )
            
            # å¦‚æœå¤‡ä»½è€—æ—¶è¿‡é•¿ï¼Œè®°å½•è­¦å‘Š
            if duration > 300:  # 5åˆ†é’Ÿ
                current_app.logger.warning(
                    f"å¤‡ä»½è€—æ—¶è¾ƒé•¿: {duration:.2f}ç§’ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿæ€§èƒ½"
                )
                
        except Exception as e:
            current_app.logger.error(f"æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
    
    def create_backup(self, backup_type: str = 'incremental', options: Dict[str, Any] = None) -> str:
        """
        åˆ›å»ºå¤‡ä»½
        
        Args:
            backup_type: å¤‡ä»½ç±»å‹ (full, incremental, snapshot)
            options: å¤‡ä»½é€‰é¡¹
            
        Returns:
            å¤‡ä»½ID
        """
        if options is None:
            options = {}
            
        # æ€§èƒ½ç›‘æ§ï¼šè®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        self.cleanup_temp_files()
        
        # ç”Ÿæˆå¤‡ä»½ID - ä½¿ç”¨ä¸Šæµ·æ—¶åŒº
        from ..models import SHANGHAI_TZ
        backup_id = f"{backup_type}_{datetime.now(SHANGHAI_TZ).strftime('%Y%m%d_%H%M%S')}"
        current_app.logger.info(f"åˆ›å»ºå¤‡ä»½ - ID: {backup_id}, ç±»å‹: {backup_type}, é€‰é¡¹: {options}")

        # åˆ›å»ºå¤‡ä»½è®°å½•
        backup_record = BackupRecord(
            backup_id=backup_id,
            backup_type=backup_type,
            status='pending'
        )
        backup_record.set_extra_data(options)
        
        try:
            current_app.logger.info("å°†å¤‡ä»½è®°å½•æäº¤æ•°æ®åº“")
            db.session.add(backup_record)
            db.session.commit()
            
            # åˆ›å»ºå–æ¶ˆæ ‡å¿—
            cancel_event = self._create_cancellation_flag(backup_id)
            
            # è·å–Flaskåº”ç”¨å®ä¾‹ç”¨äºçº¿ç¨‹ä¸­çš„åº”ç”¨ä¸Šä¸‹æ–‡
            app = current_app._get_current_object()
            
            # å¼‚æ­¥æ‰§è¡Œå¤‡ä»½ - ä¼ é€’backup_idè€Œä¸æ˜¯æ•´ä¸ªrecordå¯¹è±¡
            backup_thread = threading.Thread(
                target=self._execute_backup_async,
                args=(app, backup_id, options, start_time),
                name=f"backup-{backup_id}",
                daemon=True
            )
            
            # è®°å½•æ´»è·ƒçº¿ç¨‹
            with self._flags_lock:
                self._active_threads[backup_id] = backup_thread
            
            # å¯åŠ¨å¤‡ä»½çº¿ç¨‹
            backup_thread.start()
            current_app.logger.info(f"å¤‡ä»½ {backup_id} å¼€å§‹å¼‚æ­¥æ‰§è¡Œ")
            
            return backup_id
            
        except Exception as e:
            backup_record.status = 'failed'
            backup_record.error_message = str(e)
            backup_record.completed_at = datetime.now(SHANGHAI_TZ)
            db.session.commit()
            current_app.logger.error(f"Backup {backup_id} creation failed: {e}")
            self._cleanup_cancellation_flag(backup_id)
            raise
    
    def _execute_backup_async(self, app, backup_id: str, options: Dict[str, Any], start_time: float):
        """å¼‚æ­¥æ‰§è¡Œå¤‡ä»½æ“ä½œçš„åŒ…è£…æ–¹æ³• - ULTRALTHINKå¢å¼ºç‰ˆ"""
        
        try:
            # ä½¿ç”¨ä¼ å…¥çš„Flaskåº”ç”¨å®ä¾‹å»ºç«‹åº”ç”¨ä¸Šä¸‹æ–‡
            with app.app_context():
                # è·å–æ–°çš„æ•°æ®åº“ä¼šè¯ï¼Œé¿å…çº¿ç¨‹é—´å…±äº«
                from .. import db
                
                # é‡æ–°æŸ¥è¯¢å¤‡ä»½è®°å½•ä»¥è·å–çº¿ç¨‹æœ¬åœ°çš„å®ä¾‹
                backup_record_local = BackupRecord.query.filter_by(backup_id=backup_id).first()
                if not backup_record_local:
                    app.logger.error(f"Backup record {backup_id} not found in async thread")
                    return
                
                # å…³é”®ä¿®å¤ï¼šè®¾ç½®å¿ƒè·³æœºåˆ¶é˜²æ­¢çŠ¶æ€å¡æ­»
                self._setup_backup_heartbeat(backup_record_local)
                
                # æ‰§è¡Œå¤‡ä»½
                self._execute_backup(backup_record_local, options)
                
                # ç¡®ä¿çŠ¶æ€æ­£ç¡®æ›´æ–°ä¸ºcompleted
                self._ensure_backup_completion(backup_record_local)
                
                # æ€§èƒ½ç›‘æ§ï¼šè®°å½•å®Œæˆæ—¶é—´å’Œæ€§èƒ½æŒ‡æ ‡
                backup_type = backup_record_local.backup_type if backup_record_local else 'unknown'
                self.monitor_backup_performance(start_time, backup_id, backup_type)
                
                app.logger.info(f"âœ… å¤‡ä»½ {backup_id} å¼‚æ­¥æ‰§è¡Œå®Œæˆ")
                
        except Exception as e:
            self._handle_backup_failure(app, backup_id, str(e))
        finally:
            # æ¸…ç†å–æ¶ˆæ ‡å¿—å’Œçº¿ç¨‹è®°å½•
            self._cleanup_cancellation_flag(backup_id)
    
    def _setup_backup_heartbeat(self, backup_record: BackupRecord):
        """è®¾ç½®å¤‡ä»½å¿ƒè·³æœºåˆ¶ï¼Œé˜²æ­¢çŠ¶æ€å¡æ­»"""
        backup_record.status = 'running'
        backup_record.started_at = datetime.now(SHANGHAI_TZ)
        backup_record.last_heartbeat = datetime.now(SHANGHAI_TZ)
        db.session.commit()
        current_app.logger.info(f"ğŸ”„ å¤‡ä»½ {backup_record.backup_id} å¿ƒè·³å·²å¯åŠ¨")
    
    def _ensure_backup_completion(self, backup_record: BackupRecord):
        """ç¡®ä¿å¤‡ä»½æ­£ç¡®æ ‡è®°ä¸ºå®ŒæˆçŠ¶æ€"""
        try:
            # åŒé‡æ£€æŸ¥ï¼šç¡®ä¿å¤‡ä»½æ–‡ä»¶ç¡®å®å­˜åœ¨
            if backup_record.file_path and os.path.exists(backup_record.file_path):
                backup_record.status = 'completed'
                backup_record.completed_at = datetime.now(SHANGHAI_TZ)
                backup_record.last_heartbeat = datetime.now(SHANGHAI_TZ)
                db.session.commit()
                current_app.logger.info(f"âœ… å¤‡ä»½ {backup_record.backup_id} çŠ¶æ€å·²ç¡®è®¤ä¸ºcompleted")
            else:
                raise Exception(f"å¤‡ä»½æ–‡ä»¶æœªæ‰¾åˆ°: {backup_record.file_path}")
        except Exception as e:
            current_app.logger.error(f"ç¡®è®¤å¤‡ä»½å®ŒæˆçŠ¶æ€å¤±è´¥: {e}")
            raise
    
    def _handle_backup_failure(self, app, backup_id: str, error_message: str):
        """å¤„ç†å¤‡ä»½å¤±è´¥çš„ç»Ÿä¸€æ–¹æ³•"""
        try:
            with app.app_context():
                from .. import db
                backup_record_local = BackupRecord.query.filter_by(backup_id=backup_id).first()
                if backup_record_local:
                    backup_record_local.status = 'failed'
                    backup_record_local.error_message = error_message
                    backup_record_local.completed_at = datetime.now(SHANGHAI_TZ)
                    backup_record_local.last_heartbeat = datetime.now(SHANGHAI_TZ)
                    db.session.commit()
                    app.logger.error(f"âŒ å¤‡ä»½ {backup_id} æ ‡è®°ä¸ºå¤±è´¥: {error_message}")
        except Exception as inner_e:
            app.logger.error(f"å¤„ç†å¤‡ä»½å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {inner_e}")
    
    def _execute_backup(self, backup_record: BackupRecord, options: Dict[str, Any]):
        """æ‰§è¡Œå¤‡ä»½æ“ä½œ"""
        backup_record.status = 'running'
        backup_record.started_at = datetime.now(SHANGHAI_TZ)
        db.session.commit()
        
        try:
            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            if self._is_cancelled(backup_record.backup_id):
                raise InterruptedError("Backup cancelled by user")
            
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = self.backup_base_dir / 'snapshots' / backup_record.backup_id
            backup_dir.mkdir(exist_ok=True)
            
            files_count = 0
            databases_count = 0
            
            # å¤‡ä»½æ•°æ®åº“
            if options.get('include_database', True):
                # æ£€æŸ¥å–æ¶ˆæ ‡å¿—
                if self._is_cancelled(backup_record.backup_id):
                    raise InterruptedError("Backup cancelled during database backup")
                
                db_backup_path = self._backup_database(backup_dir, backup_record.backup_id)
                if db_backup_path:
                    databases_count += 1
                    current_app.logger.info(f"Database backed up to {db_backup_path}")
            
            # å¤‡ä»½æ–‡ä»¶
            if options.get('include_files', True):
                # æ£€æŸ¥å–æ¶ˆæ ‡å¿—
                if self._is_cancelled(backup_record.backup_id):
                    raise InterruptedError("Backup cancelled during file backup")
                
                files_count = self._backup_files(backup_dir, backup_record.backup_type, options, backup_record.backup_id)
                current_app.logger.info(f"Backed up {files_count} files")
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å‹ç¼©å‰è¢«å–æ¶ˆ
            if self._is_cancelled(backup_record.backup_id):
                raise InterruptedError("Backup cancelled before archiving")
            
            # åˆ›å»ºå‹ç¼©åŒ…
            archive_path = self._create_archive(backup_dir)
            
            # è®¡ç®—æ ¡éªŒå’Œ
            checksum = self._calculate_checksum(archive_path)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = archive_path.stat().st_size
            compressed_size = file_size  # å·²ç»æ˜¯å‹ç¼©åçš„å¤§å°
            
            # ä¸Šä¼ åˆ°å­˜å‚¨åç«¯
            storage_info = self.storage_manager.store_backup(archive_path, backup_record.backup_id)
            
            # æ›´æ–°å¤‡ä»½è®°å½•
            backup_record.status = 'completed'
            backup_record.completed_at = datetime.now(SHANGHAI_TZ)
            backup_record.file_path = str(archive_path)
            backup_record.file_size = file_size
            backup_record.compressed_size = compressed_size
            backup_record.checksum = checksum
            backup_record.files_count = files_count
            backup_record.databases_count = databases_count
            backup_record.set_storage_providers(storage_info)
            
            # è®¡ç®—å‹ç¼©æ¯”
            if backup_record.backup_type == 'full':
                original_size = self._calculate_original_size(backup_dir)
                if original_size > 0:
                    backup_record.compression_ratio = compressed_size / original_size
            
            db.session.commit()
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(backup_dir, ignore_errors=True)
            
            current_app.logger.info(f"Backup {backup_record.backup_id} completed successfully")
            
        except InterruptedError as e:
            # å¤‡ä»½è¢«å–æ¶ˆ - ä¿ç•™éƒ¨åˆ†å¤‡ä»½æ–‡ä»¶
            backup_record.status = 'cancelled'
            backup_record.error_message = str(e)
            # æ³¨æ„ï¼šä¸è®¾ç½®completed_atï¼Œä¿æŒä¸ºNone
            
            # å°è¯•ä¿å­˜å·²å®Œæˆçš„éƒ¨åˆ†æ•°æ®
            self._save_partial_backup(backup_record, backup_dir, files_count, databases_count)
            db.session.commit()
            
            current_app.logger.info(f"Backup {backup_record.backup_id} cancelled, partial data preserved")
            
        except Exception as e:
            # å¤‡ä»½å¤±è´¥ - æ¸…ç†æ‰€æœ‰æ–‡ä»¶
            backup_record.status = 'failed'
            backup_record.error_message = str(e)
            backup_record.completed_at = datetime.now(SHANGHAI_TZ)
            db.session.commit()
            
            # æ¸…ç†å¤±è´¥çš„å¤‡ä»½æ–‡ä»¶
            backup_dir = self.backup_base_dir / 'snapshots' / backup_record.backup_id
            if backup_dir.exists():
                shutil.rmtree(backup_dir, ignore_errors=True)
            
            current_app.logger.error(f"Backup {backup_record.backup_id} failed: {e}")
            raise
    
    def _save_partial_backup(self, backup_record: BackupRecord, backup_dir: Path, files_count: int, databases_count: int):
        """ä¿å­˜è¢«å–æ¶ˆå¤‡ä»½çš„éƒ¨åˆ†æ•°æ®"""
        try:
            if not backup_dir.exists():
                return
                
            # æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†æ–‡ä»¶å¯ä»¥ä¿å­˜
            partial_files = list(backup_dir.rglob('*'))
            if not partial_files:
                return
            
            # è®¡ç®—éƒ¨åˆ†å¤‡ä»½çš„å¤§å°
            total_size = sum(f.stat().st_size for f in partial_files if f.is_file())
            
            if total_size > 0:
                # åˆ›å»ºéƒ¨åˆ†å¤‡ä»½çš„å‹ç¼©åŒ…
                try:
                    partial_archive = backup_dir.parent / f"{backup_record.backup_id}_partial.tar.gz"
                    with tarfile.open(partial_archive, "w:gz", compresslevel=6) as tar:
                        tar.add(backup_dir, arcname=backup_dir.name)
                    
                    # æ›´æ–°è®°å½•ä¿¡æ¯
                    backup_record.file_path = str(partial_archive)
                    backup_record.file_size = partial_archive.stat().st_size
                    backup_record.compressed_size = backup_record.file_size
                    backup_record.files_count = files_count
                    backup_record.databases_count = databases_count
                    backup_record.checksum = self._calculate_checksum(partial_archive)
                    
                    # æ·»åŠ éƒ¨åˆ†å¤‡ä»½æ ‡è®°åˆ°extra_data
                    extra_data = json.loads(backup_record.extra_data) if backup_record.extra_data else {}
                    extra_data['partial_backup'] = True
                    extra_data['preservation_reason'] = 'Cancelled by user'
                    backup_record.set_extra_data(extra_data)
                    
                    current_app.logger.info(f"Partial backup saved: {partial_archive} ({total_size} bytes)")
                    
                except Exception as archive_error:
                    current_app.logger.warning(f"Failed to create partial archive: {archive_error}")
            
        except Exception as e:
            current_app.logger.warning(f"Failed to save partial backup: {e}")
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if backup_dir.exists():
                shutil.rmtree(backup_dir, ignore_errors=True)
    
    def _backup_database(self, backup_dir: Path, backup_id: str = None) -> Optional[Path]:
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            db_uri = current_app.config.get('SQLALCHEMY_DATABASE_URI', '')
            timestamp = datetime.now(SHANGHAI_TZ).strftime('%Y%m%d_%H%M%S')
            
            if db_uri.startswith('sqlite:'):
                return self._backup_sqlite(db_uri, backup_dir, timestamp)
            elif db_uri.startswith('mysql'):
                return self._backup_mysql(db_uri, backup_dir, timestamp)
            else:
                current_app.logger.warning(f"Unsupported database type for backup: {db_uri}")
                return None
                
        except Exception as e:
            current_app.logger.error(f"Database backup failed: {e}")
            raise
    
    def _backup_sqlite(self, db_uri: str, backup_dir: Path, timestamp: str) -> Optional[Path]:
        """å¤‡ä»½SQLiteæ•°æ®åº“"""
        try:
            db_path = db_uri.replace('sqlite:///', '')
            if not os.path.isabs(db_path):
                db_path = str(Path(current_app.root_path).parent / db_path)
            
            if os.path.exists(db_path):
                backup_file = backup_dir / f"database_{timestamp}.db"
                shutil.copy2(db_path, backup_file)
                
                # SQLå¯¼å‡ºå¤‡ä»½ (ä½œä¸ºé¢å¤–ä¿é™©)
                sql_backup_file = backup_dir / f"database_{timestamp}.sql"
                self._export_sqlite_to_sql(db_path, sql_backup_file)
                
                current_app.logger.info(f"SQLite database backed up: {backup_file}")
                return backup_file
            else:
                current_app.logger.error(f"SQLite database file not found: {db_path}")
                return None
                
        except Exception as e:
            current_app.logger.error(f"SQLite backup failed: {e}")
            raise
    
    def _backup_mysql(self, db_uri: str, backup_dir: Path, timestamp: str) -> Optional[Path]:
        """å¤‡ä»½MySQLæ•°æ®åº“"""
        try:
            import urllib.parse
            from sqlalchemy import create_engine, text
            
            # è§£ææ•°æ®åº“è¿æ¥ä¿¡æ¯
            parsed = urllib.parse.urlparse(db_uri)
            host = parsed.hostname or 'localhost'
            port = parsed.port or 3306
            username = parsed.username
            password = parsed.password
            database = parsed.path.lstrip('/')
            
            # ä½¿ç”¨mysqldumpå‘½ä»¤å¤‡ä»½
            backup_file = backup_dir / f"database_{timestamp}.sql"
            
            # å°è¯•ä½¿ç”¨mysqldump - ä¼˜å…ˆä½¿ç”¨Dockeræ–¹å¼
            import subprocess
            try:
                # æ£€æŸ¥æ˜¯å¦åœ¨Dockerç¯å¢ƒä¸­
                docker_container = self._detect_mysql_docker_container()
                
                if docker_container:
                    # ä½¿ç”¨Dockerå®¹å™¨å†…çš„mysqldump
                    current_app.logger.info(f"ä½¿ç”¨Dockerå®¹å™¨ {docker_container} æ‰§è¡Œmysqldump")
                    cmd = [
                        'docker', 'exec', docker_container,
                        'mysqldump',
                        '--host=localhost',  # å®¹å™¨å†…éƒ¨è¿æ¥
                        '--port=3306',
                        f'--user={username}',
                        f'--password={password}',
                        '--single-transaction',
                        '--routines',
                        '--triggers',
                        '--skip-disable-keys',  # ä¸ç”ŸæˆDISABLE/ENABLE KEYSè¯­å¥
                        '--skip-lock-tables',   # ä¸é”è¡¨
                        database
                    ]
                    env = None
                else:
                    # ä¼ ç»Ÿæ–¹å¼ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡ä¼ é€’å¯†ç ï¼ˆæ›´å®‰å…¨ï¼‰
                    cmd = [
                        'mysqldump',
                        f'--host={host}',
                        f'--port={port}',
                        f'--user={username}',
                        '--single-transaction',
                        '--routines',
                        '--triggers',
                        '--skip-disable-keys',  # ä¸ç”ŸæˆDISABLE/ENABLE KEYSè¯­å¥
                        '--skip-lock-tables',   # ä¸é”è¡¨
                        database
                    ]
                    # è®¾ç½®ç¯å¢ƒå˜é‡ä¼ é€’å¯†ç 
                    env = os.environ.copy()
                    env['MYSQL_PWD'] = password
                
                with open(backup_file, 'w', encoding='utf-8') as f:
                    result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, 
                                         check=True, text=True, env=env)
                
                current_app.logger.info(f"MySQL database backed up with mysqldump: {backup_file}")
                return backup_file
                
            except (subprocess.CalledProcessError, FileNotFoundError) as e:
                current_app.logger.warning(f"mysqldump failed, trying SQLAlchemy approach: {e}")
                
                # å›é€€åˆ°SQLAlchemyæ–¹æ³•
                return self._backup_mysql_with_sqlalchemy(db_uri, backup_dir, timestamp)
                
        except Exception as e:
            current_app.logger.error(f"MySQL backup failed: {e}")
            raise
    
    def _backup_mysql_with_sqlalchemy(self, db_uri: str, backup_dir: Path, timestamp: str) -> Optional[Path]:
        """ä½¿ç”¨SQLAlchemyå¤‡ä»½MySQLæ•°æ®åº“"""
        try:
            from sqlalchemy import text, inspect
            
            # ä½¿ç”¨ä¼˜åŒ–çš„è¿æ¥æ± 
            engine = self.get_engine_with_pool(db_uri)
            inspector = inspect(engine)
            
            backup_file = backup_dir / f"database_{timestamp}.sql"
            
            # è¿æ¥ç®¡ç†ï¼šç¡®ä¿è¿æ¥è¢«æ­£ç¡®é‡Šæ”¾
            connection = None
            try:
                connection = engine.connect()
                
                with open(backup_file, 'w', encoding='utf-8') as f:
                    f.write(f"-- MySQL Database Backup\\n")
                    f.write(f"-- Generated at: {datetime.now(SHANGHAI_TZ)}\\n\\n")
                    
                    # å¤‡ä»½æ¯ä¸ªè¡¨
                    for table_name in inspector.get_table_names():
                        f.write(f"\\n-- Table: {table_name}\\n")
                        
                        # è·å–è¡¨ç»“æ„
                        f.write(f"DROP TABLE IF EXISTS `{table_name}`;\\n")
                        
                        # è·å–åˆ›å»ºè¡¨è¯­å¥ (ç®€åŒ–ç‰ˆ)
                        result = connection.execute(text(f"SHOW CREATE TABLE `{table_name}`"))
                        create_stmt = result.fetchone()[1]
                        f.write(f"{create_stmt};\\n\\n")
                        
                        # ä¼˜åŒ–ï¼šä½¿ç”¨æµå¼å¤„ç†é¿å…å†…å­˜æº¢å‡º
                        data_result = connection.execute(text(f"SELECT * FROM `{table_name}`"))
                        
                        # è·å–åˆ—ä¿¡æ¯
                        columns = data_result.keys()
                        if columns:
                            col_names = '`, `'.join(columns)
                            
                            # æ‰¹é‡å¤„ç†æ•°æ®ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è¡Œåˆ°å†…å­˜
                            batch_size = 1000  # æ¯æ‰¹å¤„ç†1000è¡Œ
                            batch_rows = []
                            has_data = False
                            
                            for row in data_result:
                                if not has_data:
                                    f.write(f"INSERT INTO `{table_name}` (`{col_names}`) VALUES\\n")
                                    has_data = True
                                
                                batch_rows.append(row)
                                
                                if len(batch_rows) >= batch_size:
                                    self._write_batch_insert(f, batch_rows, False)
                                    batch_rows = []
                            
                            # å¤„ç†æœ€åä¸€æ‰¹æ•°æ®
                            if batch_rows:
                                self._write_batch_insert(f, batch_rows, True)
                            elif has_data:
                                # å¦‚æœæœ‰æ•°æ®ä½†æ²¡æœ‰å‰©ä½™æ‰¹æ¬¡ï¼Œéœ€è¦æ·»åŠ åˆ†å·
                                f.write(';\\n\\n')
            
                current_app.logger.info(f"MySQL database backed up with SQLAlchemy: {backup_file}")
                return backup_file
                
            finally:
                # ç¡®ä¿è¿æ¥è¢«æ­£ç¡®å…³é—­
                if connection:
                    try:
                        connection.close()
                    except Exception as e:
                        current_app.logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
            
        except Exception as e:
            current_app.logger.error(f"MySQL SQLAlchemy backup failed: {e}")
            raise
    
    def _write_batch_insert(self, f, batch_rows, is_last_batch):
        """
        å†™å…¥æ‰¹é‡INSERTè¯­å¥çš„æ•°æ®è¡Œ
        
        Args:
            f: æ–‡ä»¶å¯¹è±¡
            batch_rows: æ‰¹é‡æ•°æ®è¡Œ
            is_last_batch: æ˜¯å¦ä¸ºæœ€åä¸€æ‰¹
        """
        try:
            for i, row in enumerate(batch_rows):
                values = []
                for value in row:
                    if value is None:
                        values.append('NULL')
                    elif isinstance(value, str):
                        # è½¬ä¹‰SQLå­—ç¬¦ï¼Œé˜²æ­¢SQLæ³¨å…¥
                        escaped = value.replace("\\", "\\\\").replace("'", "\\'")
                        values.append(f"'{escaped}'")
                    elif isinstance(value, (int, float)):
                        values.append(str(value))
                    elif isinstance(value, datetime):
                        values.append(f"'{value.isoformat()}'")
                    else:
                        values.append(f"'{str(value)}'")
                
                f.write(f"({', '.join(values)})")
                
                # å†³å®šæ˜¯å¦æ·»åŠ é€—å·æˆ–åˆ†å·
                if i == len(batch_rows) - 1:  # å½“å‰æ‰¹æ¬¡çš„æœ€åä¸€è¡Œ
                    if is_last_batch:
                        f.write(';\\n\\n')  # æœ€åä¸€æ‰¹çš„æœ€åä¸€è¡Œï¼Œæ·»åŠ åˆ†å·
                    else:
                        f.write(',\\n')     # ä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œæ·»åŠ é€—å·
                else:
                    f.write(',\\n')         # ä¸æ˜¯å½“å‰æ‰¹æ¬¡çš„æœ€åä¸€è¡Œï¼Œæ·»åŠ é€—å·
                    
        except Exception as e:
            current_app.logger.error(f"Error writing batch insert: {e}")
            raise
    
    def _export_sqlite_to_sql(self, db_path: str, output_file: Path):
        """å¯¼å‡ºSQLiteä¸ºSQLæ–‡ä»¶"""
        try:
            conn = sqlite3.connect(db_path)
            with open(output_file, 'w', encoding='utf-8') as f:
                for line in conn.iterdump():
                    f.write(f'{line}\n')
            conn.close()
        except Exception as e:
            current_app.logger.error(f"SQLite export failed: {e}")
            raise
    
    def _backup_files(self, backup_dir: Path, backup_type: str, options: Dict[str, Any], backup_id: str = None) -> int:
        """å¤‡ä»½æ–‡ä»¶ç³»ç»Ÿ"""
        files_count = 0
        
        try:
            # è·å–éœ€è¦å¤‡ä»½çš„ç›®å½•
            default_include_patterns = [
                'uploads/**/*',
                'instance/**/*',
                'config/**/*', 
                'logs/**/*',
                'migrations/**/*',
                'app/**/*.py',  # å¤‡ä»½åº”ç”¨ä»£ç 
                'requirements.txt',
                'run.py'
            ]
            
            default_exclude_patterns = [
                '*.pyc',
                '__pycache__/*',
                'backups/*',
                '.git/*',
                '*.tmp',
                '*.swp'
            ]
            
            include_patterns = options.get('include_patterns') or default_include_patterns
            exclude_patterns = options.get('exclude_patterns') or default_exclude_patterns
            
            app_root = Path(current_app.root_path).parent
            files_backup_dir = backup_dir / 'files'
            files_backup_dir.mkdir(exist_ok=True)
            
            if backup_type == 'incremental':
                files_count = self._incremental_backup(app_root, files_backup_dir, include_patterns, exclude_patterns)
            else:
                files_count = self._full_backup(app_root, files_backup_dir, include_patterns, exclude_patterns)
                
            return files_count
            
        except Exception as e:
            current_app.logger.error(f"File backup failed: {e}")
            raise
    
    def _full_backup(self, source_dir: Path, backup_dir: Path, include_patterns: List[str], exclude_patterns: List[str]) -> int:
        """å…¨é‡æ–‡ä»¶å¤‡ä»½"""
        files_count = 0
        
        for pattern in include_patterns:
            for file_path in source_dir.glob(pattern):
                if file_path.is_file() and not self._should_exclude(file_path, exclude_patterns, source_dir):
                    # åˆ›å»ºç›¸å¯¹è·¯å¾„ç»“æ„
                    rel_path = file_path.relative_to(source_dir)
                    target_path = backup_dir / rel_path
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # å¤åˆ¶æ–‡ä»¶
                    shutil.copy2(file_path, target_path)
                    files_count += 1
        return files_count
    
    def _incremental_backup(self, source_dir: Path, backup_dir: Path, include_patterns: List[str], exclude_patterns: List[str]) -> int:
        """å¢é‡æ–‡ä»¶å¤‡ä»½"""
        files_count = 0
        
        # è·å–ä¸Šæ¬¡å¤‡ä»½çš„æ¸…å•
        last_manifest = self._get_last_backup_manifest()
        current_manifest = {}
        
        for pattern in include_patterns:
            for file_path in source_dir.glob(pattern):
                if file_path.is_file() and not self._should_exclude(file_path, exclude_patterns, source_dir):
                    # è®¡ç®—æ–‡ä»¶hash
                    file_hash = self._calculate_file_hash(file_path)
                    rel_path = str(file_path.relative_to(source_dir))
                    
                    current_manifest[rel_path] = {
                        'hash': file_hash,
                        'mtime': file_path.stat().st_mtime,
                        'size': file_path.stat().st_size
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤‡ä»½
                    if rel_path not in last_manifest or last_manifest[rel_path]['hash'] != file_hash:
                        target_path = backup_dir / rel_path
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, target_path)
                        files_count += 1
        
        # ä¿å­˜å½“å‰æ¸…å•
        self._save_backup_manifest(current_manifest)
        
        return files_count
    
    def _should_exclude(self, file_path: Path, exclude_patterns: List[str], source_dir: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥æ’é™¤"""
        import fnmatch
        
        rel_path = file_path.relative_to(source_dir)
        rel_path_str = str(rel_path).replace('\\', '/')  # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
        
        for pattern in exclude_patterns:
            # ä½¿ç”¨fnmatchè¿›è¡Œæ¨¡å¼åŒ¹é…
            if fnmatch.fnmatch(rel_path_str, pattern):
                return True
            # å¯¹äºç›®å½•æ¨¡å¼ï¼Œæ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨è¯¥ç›®å½•ä¸‹
            if pattern.endswith('/*') and rel_path_str.startswith(pattern[:-2] + '/'):
                return True
        return False
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _get_last_backup_manifest(self) -> Dict[str, Any]:
        """è·å–ä¸Šæ¬¡å¤‡ä»½çš„æ¸…å•"""
        try:
            manifest_file = self.backup_base_dir / 'last_manifest.json'
            if manifest_file.exists():
                with open(manifest_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            current_app.logger.warning(f"Failed to load last backup manifest: {e}")
        return {}
    
    def _save_backup_manifest(self, manifest: Dict[str, Any]):
        """ä¿å­˜å¤‡ä»½æ¸…å•"""
        try:
            manifest_file = self.backup_base_dir / 'last_manifest.json'
            with open(manifest_file, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, indent=2)
        except Exception as e:
            current_app.logger.error(f"Failed to save backup manifest: {e}")
    
    def _create_archive(self, backup_dir: Path) -> Path:
        """åˆ›å»ºå¤‡ä»½å‹ç¼©åŒ…"""
        try:
            archive_path = backup_dir.parent / f"{backup_dir.name}.tar.gz"
            
            with tarfile.open(archive_path, "w:gz", compresslevel=6) as tar:
                tar.add(backup_dir, arcname=backup_dir.name)
                
            return archive_path
            
        except Exception as e:
            current_app.logger.error(f"Archive creation failed: {e}")
            raise
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶SHA-256æ ¡éªŒå’Œ"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def _calculate_original_size(self, backup_dir: Path) -> int:
        """è®¡ç®—åŸå§‹æ–‡ä»¶å¤§å°"""
        total_size = 0
        for file_path in backup_dir.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size
    
    def get_backup_records(self, page: int = 1, per_page: int = 20, backup_type: str = None, status: str = None) -> Dict[str, Any]:
        """è·å–å¤‡ä»½è®°å½•åˆ—è¡¨"""
        query = BackupRecord.query.order_by(BackupRecord.created_at.desc())
        
        if backup_type:
            query = query.filter(BackupRecord.backup_type == backup_type)
        if status:
            query = query.filter(BackupRecord.status == status)
            
        pagination = query.paginate(page=page, per_page=per_page, error_out=False)
        
        return {
            'records': [record.to_dict() for record in pagination.items],
            'total': pagination.total,
            'pages': pagination.pages,
            'current_page': page,
            'per_page': per_page,
            'has_next': pagination.has_next,
            'has_prev': pagination.has_prev
        }
    
    def get_backup_record(self, backup_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å•ä¸ªå¤‡ä»½è®°å½•"""
        record = BackupRecord.query.filter_by(backup_id=backup_id).first()
        return record.to_dict() if record else None
    
    def delete_backup(self, backup_id: str) -> bool:
        """åˆ é™¤å¤‡ä»½"""
        try:
            record = BackupRecord.query.filter_by(backup_id=backup_id).first()
            if not record:
                return False
            
            # åˆ é™¤å­˜å‚¨åç«¯çš„æ–‡ä»¶
            if record.storage_providers:
                self.storage_manager.delete_backup(backup_id, record.storage_providers)
            
            # åˆ é™¤æœ¬åœ°æ–‡ä»¶
            if record.file_path and os.path.exists(record.file_path):
                os.remove(record.file_path)
            
            # åˆ é™¤æ•°æ®åº“è®°å½•
            db.session.delete(record)
            db.session.commit()
            
            current_app.logger.info(f"Backup {backup_id} deleted successfully")
            return True
            
        except Exception as e:
            current_app.logger.error(f"Failed to delete backup {backup_id}: {e}")
            db.session.rollback()
            return False
    
    def cleanup_expired_backups(self) -> Dict[str, int]:
        """æ¸…ç†è¿‡æœŸçš„å¤‡ä»½"""
        try:
            # è·å–ä¿ç•™å¤©æ•°é…ç½®
            retention_config = BackupConfig.query.filter_by(config_key='backup_retention_days').first()
            retention_days = int(retention_config.config_value) if retention_config else 30
            
            # è®¡ç®—è¿‡æœŸæ—¶é—´
            expiry_date = datetime.now(SHANGHAI_TZ) - timedelta(days=retention_days)
            
            # æŸ¥è¯¢è¿‡æœŸçš„å¤‡ä»½
            expired_records = BackupRecord.query.filter(
                BackupRecord.created_at < expiry_date,
                BackupRecord.status == 'completed'
            ).all()
            
            deleted_count = 0
            failed_count = 0
            
            for record in expired_records:
                if self.delete_backup(record.backup_id):
                    deleted_count += 1
                else:
                    failed_count += 1
            
            current_app.logger.info(f"Cleanup completed: {deleted_count} backups deleted, {failed_count} failed")
            
            return {
                'deleted_count': deleted_count,
                'failed_count': failed_count,
                'total_expired': len(expired_records)
            }
            
        except Exception as e:
            current_app.logger.error(f"Cleanup failed: {e}")
            return {
                'deleted_count': 0,
                'failed_count': 0,
                'total_expired': 0,
                'error': str(e)
            }
    
    def get_backup_statistics(self) -> Dict[str, Any]:
        """è·å–å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # å¯¼å…¥ä¸Šæµ·æ—¶åŒº
            from ..models import SHANGHAI_TZ
            
            total_backups = BackupRecord.query.count()
            completed_backups = BackupRecord.query.filter_by(status='completed').count()
            failed_backups = BackupRecord.query.filter_by(status='failed').count()
            
            # æœ€è¿‘30å¤©çš„å¤‡ä»½
            recent_date = datetime.now(SHANGHAI_TZ) - timedelta(days=30)
            recent_backups = BackupRecord.query.filter(BackupRecord.created_at >= recent_date).count()
            
            # å­˜å‚¨ä½¿ç”¨é‡ - åªç»Ÿè®¡å®é™…æœ‰æ–‡ä»¶å¤§å°æ•°æ®çš„å¤‡ä»½ï¼ˆæ— è®ºçŠ¶æ€å¦‚ä½•ï¼‰
            total_size_result = db.session.query(db.func.sum(BackupRecord.file_size)).filter(
                BackupRecord.file_size.isnot(None), 
                BackupRecord.file_size > 0
            ).scalar()
            total_size = int(total_size_result) if total_size_result else 0
            
            compressed_size_result = db.session.query(db.func.sum(BackupRecord.compressed_size)).filter(
                BackupRecord.compressed_size.isnot(None),
                BackupRecord.compressed_size > 0
            ).scalar()
            compressed_size = int(compressed_size_result) if compressed_size_result else 0
            
            # å¤‡ä»½ç±»å‹åˆ†å¸ƒ
            backup_types = db.session.query(
                BackupRecord.backup_type,
                db.func.count(BackupRecord.id)
            ).filter_by(status='completed').group_by(BackupRecord.backup_type).all()
            
            return {
                'total_backups': total_backups,
                'completed_backups': completed_backups,
                'failed_backups': failed_backups,
                'recent_backups': recent_backups,
                'success_rate': round(completed_backups / total_backups * 100, 2) if total_backups > 0 else 0,
                'total_storage_size': total_size,
                'compressed_storage_size': compressed_size,
                'compression_ratio': round(compressed_size / total_size, 3) if total_size > 0 else 0,
                'backup_types': dict(backup_types)
            }
            
        except Exception as e:
            current_app.logger.error(f"Failed to get backup statistics: {e}")
            return {}
    
    def _detect_mysql_docker_container(self) -> Optional[str]:
        """æ£€æµ‹MySQL Dockerå®¹å™¨"""
        try:
            import subprocess
            # æŸ¥æ‰¾è¿è¡Œä¸­çš„MySQLå®¹å™¨
            cmd = ['docker', 'ps', '--filter', 'ancestor=mysql', '--format', '{{.Names}}']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                container_name = result.stdout.strip().split('\n')[0]
                current_app.logger.info(f"æ£€æµ‹åˆ°MySQL Dockerå®¹å™¨: {container_name}")
                return container_name
            
            # å°è¯•é€šç”¨çš„å®¹å™¨åç§°
            common_names = ['blog-mysql', 'mysql', 'db', 'database']
            for name in common_names:
                check_cmd = ['docker', 'ps', '--filter', f'name={name}', '--format', '{{.Names}}']
                check_result = subprocess.run(check_cmd, capture_output=True, text=True, timeout=5)
                
                if check_result.returncode == 0 and name in check_result.stdout:
                    current_app.logger.info(f"æ‰¾åˆ°MySQLå®¹å™¨: {name}")
                    return name
            
            return None
            
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            current_app.logger.warning(f"Dockeræ£€æµ‹å¤±è´¥: {e}")
            return None