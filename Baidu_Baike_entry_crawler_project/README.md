# ç™¾åº¦ç™¾ç§‘æ‰¹é‡æ•°æ®çˆ¬è™« / Baidu Baike Batch Data Crawler

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## ä¸­æ–‡

### é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªåŸºäºPythonçš„ç™¾åº¦ç™¾ç§‘æ‰¹é‡æ•°æ®çˆ¬è™«é¡¹ç›®ï¼Œæ”¯æŒ10ä¸‡çº§åˆ«æ•°æ®é‡‡é›†ã€‚è¯¥çˆ¬è™«é‡‡ç”¨å¤šçº¿ç¨‹æ¶æ„ï¼Œæ”¯æŒé™æ€ä»£ç†å’ŒåŠ¨æ€ä»£ç†ä¸¤ç§æ¨¡å¼ï¼Œèƒ½å¤Ÿé«˜æ•ˆç¨³å®šåœ°çˆ¬å–ç™¾åº¦ç™¾ç§‘äººç‰©æ¡ç›®ä¿¡æ¯ã€‚

### ä¸»è¦ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å¤šçº¿ç¨‹**ï¼šæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘çˆ¬å–ï¼Œå¯å¤„ç†10ä¸‡çº§åˆ«æ•°æ®
- ğŸ”„ **åŒä»£ç†æ¨¡å¼**ï¼šæ”¯æŒé™æ€ä»£ç†æ± å’ŒåŠ¨æ€ä»£ç†APIä¸¤ç§æ¨¡å¼
- ğŸ›¡ï¸ **åçˆ¬è™«ç­–ç•¥**ï¼šå†…ç½®è¯·æ±‚å»¶è¿Ÿã€ä»£ç†è½®æ¢ã€é‡è¯•æœºåˆ¶
- ğŸ“Š **å®æ—¶ç›‘æ§**ï¼šæä¾›çˆ¬å–è¿›åº¦å®æ—¶æ˜¾ç¤ºå’Œç»Ÿè®¡ä¿¡æ¯
- ğŸ’¾ **æ•°æ®æŒä¹…åŒ–**ï¼šæ”¯æŒCSVæ ¼å¼è¾“å‡ºå’Œä¸´æ—¶JSONç¼“å­˜
- ğŸ”§ **çµæ´»é…ç½®**ï¼šå¯è‡ªå®šä¹‰çº¿ç¨‹æ•°ã€å»¶è¿Ÿæ—¶é—´ã€é‡è¯•æ¬¡æ•°ç­‰å‚æ•°

### æŠ€æœ¯æ¶æ„

- **è¯­è¨€**ï¼šPython 3.x
- **ä¸»è¦ä¾èµ–**ï¼šrequestsã€BeautifulSoup4ã€pandasã€tqdm
- **å¹¶å‘æ¨¡å‹**ï¼šåŸºäºThreadPoolExecutorçš„å¤šçº¿ç¨‹æ¶æ„
- **ä»£ç†ç®¡ç†**ï¼šçº¿ç¨‹å®‰å…¨çš„ä»£ç†æ± ç®¡ç†å™¨

### æ ¸å¿ƒåŠŸèƒ½

#### 1. äººç‰©ä¿¡æ¯æå–
çˆ¬è™«èƒ½å¤Ÿä»ç™¾åº¦ç™¾ç§‘æå–ä»¥ä¸‹äººç‰©ä¿¡æ¯ï¼š
- åŸºæœ¬ä¿¡æ¯ï¼šä¸­æ–‡åã€å¤–æ–‡åã€åˆ«åã€æ€§åˆ«ã€å›½ç±ã€æ°‘æ—ã€ç±è´¯
- ä¸ªäººèƒŒæ™¯ï¼šå‡ºç”Ÿæ—¥æœŸã€é€ä¸–æ—¥æœŸã€æ¯•ä¸šé™¢æ ¡ã€æ”¿æ²»é¢è²Œ
- èŒä¸šä¿¡æ¯ï¼šèŒä¸šã€èŒç§°ã€æ‹…ä»»èŒåŠ¡ã€å†›è¡”æ™‹å‡
- æˆå°±ä½œå“ï¼šä»£è¡¨ä½œå“ã€æˆå°±ã€äººç‰©è¯„ä»·
- ç¤¾ä¼šå…³ç³»ï¼šå®¶åº­æˆå‘˜ã€ä¸ªäººç”Ÿæ´»
- å±¥å†ä¿¡æ¯ï¼šäººç‰©å±¥å†

#### 2. åŠ¨æ€ä»£ç†æ”¯æŒ
- é€šè¿‡APIæ¥å£è‡ªåŠ¨è·å–ä»£ç†
- æ”¯æŒå¤šç§APIå“åº”æ ¼å¼è§£æ
- ä»£ç†å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨åˆ‡æ¢
- ä»£ç†ä½¿ç”¨ç»Ÿè®¡å’Œæ•…éšœæ¢å¤

#### 3. æ™ºèƒ½åçˆ¬è™«
- éšæœºè¯·æ±‚å»¶è¿Ÿ
- User-Agentè½®æ¢
- ä»£ç†IPè½®æ¢
- è¯·æ±‚å¤±è´¥é‡è¯•
- 403é”™è¯¯æ™ºèƒ½å¤„ç†

### æ–‡ä»¶ç»“æ„

```
â”œâ”€â”€ config.py                 # é…ç½®æ–‡ä»¶
â”œâ”€â”€ multithreaded_scraper.py   # å¤šçº¿ç¨‹çˆ¬è™«ä¸»ç¨‹åº
â”œâ”€â”€ dynamic_proxy.py           # åŠ¨æ€ä»£ç†ç®¡ç†å™¨
â”œâ”€â”€ run_dynamic_scraper.py     # çˆ¬è™«å¯åŠ¨å™¨
â”œâ”€â”€ names.csv                  # äººååˆ—è¡¨ï¼ˆ10ä¸‡æ¡æ•°æ®ï¼‰
â”œâ”€â”€ scraper.log               # è¿è¡Œæ—¥å¿—
â””â”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

### å¿«é€Ÿå¼€å§‹

#### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…Pythonä¾èµ–
pip install requests beautifulsoup4 pandas tqdm ftfy
```

#### 2. é…ç½®è®¾ç½®

ç¼–è¾‘ `config.py` æ–‡ä»¶ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š

```python
# å¤šçº¿ç¨‹é…ç½®
NUM_THREADS = 8              # çº¿ç¨‹æ•°é‡
MAX_RETRIES_PER_NAME = 2     # æœ€å¤§é‡è¯•æ¬¡æ•°
PROXY_POOL_SIZE = 32         # ä»£ç†æ± å¤§å°

# è¯·æ±‚é…ç½®
REQUEST_TIMEOUT = 20         # è¯·æ±‚è¶…æ—¶æ—¶é—´
MIN_DELAY = 2.0             # æœ€å°å»¶è¿Ÿ
MAX_DELAY = 4.0             # æœ€å¤§å»¶è¿Ÿ
```

#### 3. è¿è¡Œçˆ¬è™«

```bash
# è¿è¡ŒåŠ¨æ€ä»£ç†çˆ¬è™«
python run_dynamic_scraper.py
```

#### 4. æŸ¥çœ‹ç»“æœ

çˆ¬å–å®Œæˆåï¼Œæ•°æ®å°†ä¿å­˜åœ¨ `baike_data_multithreaded.csv` æ–‡ä»¶ä¸­ã€‚

### é…ç½®è¯´æ˜

#### åŸºæœ¬é…ç½®
- `PEOPLE_LIST_FILE`: äººååˆ—è¡¨æ–‡ä»¶è·¯å¾„
- `OUTPUT_FILE`: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
- `NUM_THREADS`: å¹¶å‘çº¿ç¨‹æ•°é‡

#### ä»£ç†é…ç½®
- `PROXY_POOL_SIZE`: ä»£ç†æ± å¤§å°
- `PROXY_TIMEOUT`: ä»£ç†è·å–è¶…æ—¶æ—¶é—´
- `DYNAMIC_PROXY_CONFIG`: åŠ¨æ€ä»£ç†APIé…ç½®

#### è¯·æ±‚é…ç½®
- `REQUEST_TIMEOUT`: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´
- `MIN_DELAY/MAX_DELAY`: è¯·æ±‚é—´éš”å»¶è¿ŸèŒƒå›´

### ç›‘æ§å’Œæ—¥å¿—

é¡¹ç›®æä¾›äº†å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½ï¼š

- **å®æ—¶è¿›åº¦æ˜¾ç¤º**ï¼šä½¿ç”¨tqdmæ˜¾ç¤ºçˆ¬å–è¿›åº¦
- **ç»Ÿè®¡ä¿¡æ¯**ï¼šæˆåŠŸç‡ã€å¤±è´¥ç‡ã€ä»£ç†ä½¿ç”¨æƒ…å†µ
- **æ—¥å¿—è®°å½•**ï¼šè¯¦ç»†çš„è¿è¡Œæ—¥å¿—ä¿å­˜åœ¨ `scraper.log`
- **é”™è¯¯è¿½è¸ª**ï¼šè®°å½•æ‰€æœ‰å¼‚å¸¸å’Œé”™è¯¯ä¿¡æ¯

### æ³¨æ„äº‹é¡¹

1. **åˆè§„ä½¿ç”¨**ï¼šè¯·éµå®ˆrobots.txtåè®®å’Œç›¸å…³æ³•å¾‹æ³•è§„
2. **é¢‘ç‡æ§åˆ¶**ï¼šå»ºè®®è®¾ç½®åˆç†çš„è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
3. **ä»£ç†è´¨é‡**ï¼šå»ºè®®ä½¿ç”¨é«˜è´¨é‡çš„ä»£ç†æœåŠ¡ï¼Œç¡®ä¿çˆ¬å–ç¨³å®šæ€§
4. **æ•°æ®å¤‡ä»½**ï¼šå®šæœŸå¤‡ä»½çˆ¬å–çš„æ•°æ®ï¼Œé¿å…æ„å¤–ä¸¢å¤±

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä»£ç†æ± å¤§å°**ï¼šç¡®ä¿ä»£ç†æ•°é‡æ˜¯çº¿ç¨‹æ•°çš„3-4å€
2. **è¯·æ±‚å»¶è¿Ÿ**ï¼šæ ¹æ®ä»£ç†è´¨é‡è°ƒæ•´å»¶è¿Ÿæ—¶é—´
3. **çº¿ç¨‹æ•°é‡**ï¼šæ ¹æ®æœºå™¨æ€§èƒ½å’Œä»£ç†æ•°é‡è°ƒæ•´
4. **é‡è¯•ç­–ç•¥**ï¼šåˆç†è®¾ç½®é‡è¯•æ¬¡æ•°ï¼Œé¿å…æ— æ•ˆé‡è¯•

### æ•…éšœæ’é™¤

#### å¸¸è§é—®é¢˜

1. **403é”™è¯¯é¢‘ç¹**
   - å¢åŠ è¯·æ±‚å»¶è¿Ÿæ—¶é—´
   - æ›´æ¢é«˜è´¨é‡ä»£ç†
   - å‡å°‘å¹¶å‘çº¿ç¨‹æ•°

2. **ä»£ç†è¿æ¥å¤±è´¥**
   - æ£€æŸ¥ä»£ç†æœåŠ¡å™¨çŠ¶æ€
   - éªŒè¯ä»£ç†APIé…ç½®
   - å¢åŠ ä»£ç†è¶…æ—¶æ—¶é—´

3. **æ•°æ®æå–ä¸å®Œæ•´**
   - æ£€æŸ¥ç½‘é¡µç»“æ„å˜åŒ–
   - æ›´æ–°CSSé€‰æ‹©å™¨
   - éªŒè¯æ•°æ®æ¸…ç†é€»è¾‘

### è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œè¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾ã€‚

---

## English

### Project Overview

This is a Python-based Baidu Baike (Baidu Encyclopedia) batch data crawler project that supports data collection at the scale of 100,000 entries. The crawler adopts a multi-threaded architecture and supports both static proxy pools and dynamic proxy APIs for efficient and stable crawling of Baidu Baike biographical entries.

### Key Features

- ğŸš€ **High-Performance Multi-threading**: Supports concurrent crawling with multiple threads, capable of handling 100K-level data
- ğŸ”„ **Dual Proxy Modes**: Supports both static proxy pools and dynamic proxy APIs
- ğŸ›¡ï¸ **Anti-Bot Strategies**: Built-in request delays, proxy rotation, and retry mechanisms
- ğŸ“Š **Real-time Monitoring**: Provides real-time crawling progress display and statistics
- ğŸ’¾ **Data Persistence**: Supports CSV output format and temporary JSON caching
- ğŸ”§ **Flexible Configuration**: Customizable thread count, delay times, retry attempts, and other parameters

### Technical Architecture

- **Language**: Python 3.x
- **Main Dependencies**: requests, BeautifulSoup4, pandas, tqdm
- **Concurrency Model**: Multi-threaded architecture based on ThreadPoolExecutor
- **Proxy Management**: Thread-safe proxy pool manager

### Core Functionality

#### 1. Biographical Information Extraction
The crawler can extract the following biographical information from Baidu Baike:
- Basic Info: Chinese name, foreign name, alias, gender, nationality, ethnicity, hometown
- Personal Background: birth date, death date, alma mater, political affiliation
- Professional Info: occupation, professional title, positions held, military rank progression
- Achievements: representative works, accomplishments, character evaluation
- Social Relations: family members, personal life
- Career History: biographical timeline

#### 2. Dynamic Proxy Support
- Automatic proxy acquisition through API interfaces
- Support for parsing multiple API response formats
- Proxy health checks and automatic switching
- Proxy usage statistics and failure recovery

#### 3. Intelligent Anti-Bot Measures
- Random request delays
- User-Agent rotation
- Proxy IP rotation
- Request failure retry mechanism
- Intelligent 403 error handling

### File Structure

```
â”œâ”€â”€ config.py                 # Configuration file
â”œâ”€â”€ multithreaded_scraper.py   # Multi-threaded crawler main program
â”œâ”€â”€ dynamic_proxy.py           # Dynamic proxy manager
â”œâ”€â”€ run_dynamic_scraper.py     # Crawler launcher
â”œâ”€â”€ names.csv                  # Name list (100K entries)
â”œâ”€â”€ scraper.log               # Runtime logs
â””â”€â”€ README.md                 # Project documentation
```

### Quick Start

#### 1. Environment Setup

```bash
# Install Python dependencies
pip install requests beautifulsoup4 pandas tqdm ftfy
```

#### 2. Configuration

Edit the `config.py` file and adjust the following parameters as needed:

```python
# Multi-threading configuration
NUM_THREADS = 8              # Number of threads
MAX_RETRIES_PER_NAME = 2     # Maximum retry attempts
PROXY_POOL_SIZE = 32         # Proxy pool size

# Request configuration
REQUEST_TIMEOUT = 20         # Request timeout
MIN_DELAY = 2.0             # Minimum delay
MAX_DELAY = 4.0             # Maximum delay
```

#### 3. Run the Crawler

```bash
# Run the dynamic proxy crawler
python run_dynamic_scraper.py
```

#### 4. View Results

After crawling is complete, data will be saved in the `baike_data_multithreaded.csv` file.

### Configuration Details

#### Basic Configuration
- `PEOPLE_LIST_FILE`: Path to the name list file
- `OUTPUT_FILE`: Output CSV file path
- `NUM_THREADS`: Number of concurrent threads

#### Proxy Configuration
- `PROXY_POOL_SIZE`: Proxy pool size
- `PROXY_TIMEOUT`: Proxy acquisition timeout
- `DYNAMIC_PROXY_CONFIG`: Dynamic proxy API configuration

#### Request Configuration
- `REQUEST_TIMEOUT`: HTTP request timeout
- `MIN_DELAY/MAX_DELAY`: Request interval delay range

### Monitoring and Logging

The project provides comprehensive monitoring and logging features:

- **Real-time Progress Display**: Uses tqdm to show crawling progress
- **Statistics**: Success rate, failure rate, proxy usage statistics
- **Log Recording**: Detailed runtime logs saved in `scraper.log`
- **Error Tracking**: Records all exceptions and error information

### Important Notes

1. **Compliance**: Please comply with robots.txt protocols and relevant laws and regulations
2. **Rate Control**: It's recommended to set reasonable request delays to avoid putting pressure on servers
3. **Proxy Quality**: Use high-quality proxy services to ensure crawling stability
4. **Data Backup**: Regularly backup crawled data to prevent accidental loss

### Performance Optimization Recommendations

1. **Proxy Pool Size**: Ensure the number of proxies is 3-4 times the thread count
2. **Request Delays**: Adjust delay times based on proxy quality
3. **Thread Count**: Adjust based on machine performance and proxy availability
4. **Retry Strategy**: Set reasonable retry attempts to avoid ineffective retries

### Troubleshooting

#### Common Issues

1. **Frequent 403 Errors**
   - Increase request delay times
   - Switch to higher quality proxies
   - Reduce concurrent thread count

2. **Proxy Connection Failures**
   - Check proxy server status
   - Verify proxy API configuration
   - Increase proxy timeout duration

3. **Incomplete Data Extraction**
   - Check for webpage structure changes
   - Update CSS selectors
   - Verify data cleaning logic

### License

This project is for educational and research purposes only. Please comply with relevant laws, regulations, and website terms of use.

### Contact

For questions or suggestions, please open an issue in the repository.

---

**Disclaimer**: This tool is intended for educational and research purposes only. Users are responsible for ensuring compliance with applicable laws and website terms of service.
